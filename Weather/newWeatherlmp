from dotenv import load_dotenv
import os
import requests
import pandas as pd
import matplotlib
matplotlib.use('TkAgg') # Explicitly set backend for wider compatibility
import matplotlib.pyplot as plt
from io import StringIO
from datetime import datetime
from pathlib import Path
import questionary
import numpy as np

# --- Define Base Path (assuming the script is in trader-helper/Weather/ or similar) ---
# If this script is in 'trader-helper/Weather/', then base_dir is 'trader-helper'
try:
    # Assuming the script is one level down from the base_dir (e.g., in a 'scripts' or 'Weather' folder)
    BASE_DIR = Path(__file__).resolve().parents[1]
except NameError:
    # Fallback if __file__ is not defined (e.g., running in an interactive environment)
    # You might need to adjust this if your script is located elsewhere relative to 'trader-helper'
    BASE_DIR = Path(".").resolve() # Assumes script is run from 'trader-helper' or 'trader-helper/Weather'
    if not (BASE_DIR / "INFO").exists() and (BASE_DIR.parent / "INFO").exists():
        BASE_DIR = BASE_DIR.parent # Adjust if script is in 'Weather' and run from 'Weather'

ENV_PATH = BASE_DIR / "Weather" / ".env"
INFO_DIR = BASE_DIR / "INFO"

# --- Load credentials from .env ---
load_dotenv(dotenv_path=ENV_PATH)
USERNAME = os.getenv("WSI_USERNAME")
PROFILE = os.getenv("WSI_PROFILE")
PASSWORD = os.getenv("WSI_PASSWORD")

if not USERNAME or not PROFILE or not PASSWORD:
    print(f"Attempted to load .env from: {ENV_PATH}")
    raise EnvironmentError("❌ Missing WSI credentials. Ensure .env exists at trader-helper/Weather/.env and is correctly formatted.")

# --- Load city to ISO and location mapping ---
mapping_file = INFO_DIR / "City_to_ISO_and_Location_Mapping.csv"
if not mapping_file.exists():
    raise FileNotFoundError(f"❌ Mapping file not found: {mapping_file}")
city_location_df = pd.read_csv(mapping_file)
city_location_df.columns = city_location_df.columns.str.strip().str.title()
CITY_TO_ISO = dict(zip(city_location_df["City"], city_location_df["Iso"]))
CITY_TO_LOCATION = dict(zip(city_location_df["City"], city_location_df["Location"]))

# --- ISO to file mappings ---
ISO_LMP_FILES = {
    "CAISO": INFO_DIR / "CAISOMaxLMP.csv",
    "ERCOT": INFO_DIR / "ERCOTMaxLMP.csv",
    "PJM": INFO_DIR / "PJMMaxLMP.csv",
    "MISO": INFO_DIR / "MISOMaxLMP.csv",
    "SPP": INFO_DIR / "SPPMaxLMP.csv",
    "NYISO": INFO_DIR / "NYISOMaxLMP.csv",
    "ISO-NE": INFO_DIR / "ISONEMaxLMP.csv"
}

# --- WSI API Functions ---
def get_city_map():
    """Fetches the city to SiteId mapping from WSI."""
    url = "https://www.wsitrader.com/Services/CSVDownloadService.svc/GetCityIds"
    params = {"Account": USERNAME, "Profile": PROFILE, "Password": PASSWORD}
    try:
        response = requests.get(url, params=params, timeout=20)
        response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)
    except requests.exceptions.RequestException as e:
        raise ConnectionError(f"Failed to connect to WSI API (GetCityIds): {e}")

    df = pd.read_csv(StringIO(response.text), sep=",", engine="python", on_bad_lines='skip')
    df.columns = df.columns.str.strip()
    if not {"Station Name", "SiteId"}.issubset(df.columns):
        raise KeyError(f"Expected columns 'Station Name', 'SiteId' not found in WSI city map. Columns are: {df.columns.tolist()}")
    return dict(zip(df["Station Name"], df["SiteId"]))

def fetch_year_data(station_id, city_name, year):
    """Fetches historical daily average temperature data for a given year from WSI."""
    def fmt(d): return d.strftime("%m/%d/%Y")
    url = "https://www.wsitrader.com/Services/CSVDownloadService.svc/GetHistoricalObservations"
    params = {
        "Account": USERNAME, "Profile": PROFILE, "Password": PASSWORD,
        "CityIds[]": station_id, "StartDate": fmt(datetime(year, 1, 1)), "EndDate": fmt(datetime(year, 12, 31)),
        "HistoricalProductId": "HISTORICAL_DAILY_AVERAGE", "DataTypes[]": "temperature",
        "TempUnits": "F", "IsTemp": "true", "IsDaily": "true", "IsDisplayDates": "false"
    }
    try:
        response = requests.get(url, params=params, timeout=60) # Increased timeout for potentially large data
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"⚠️ Warning: Failed to fetch data for {city_name} year {year} from WSI: {e}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(StringIO(response.text), sep=",", engine="python", on_bad_lines='skip')
    except pd.errors.EmptyDataError:
        print(f"ℹ️ No data returned for {city_name} year {year}.")
        return pd.DataFrame()

    if df.empty or len(df.columns) < 3:
        print(f"ℹ️ Empty or malformed data for {city_name} year {year}. Columns: {df.columns.tolist() if not df.empty else 'N/A'}")
        return pd.DataFrame()

    df.columns = df.columns.str.strip()
    # Ensure the renaming is robust to potential slight column name variations
    # The first column is Date, next two are temperature readings.
    date_col_name = df.columns[0]
    min_temp_col_name = df.columns[1]
    max_temp_col_name = df.columns[2]

    df = df.rename(columns={date_col_name: "Date", min_temp_col_name: "Min Temp", max_temp_col_name: "Max Temp"})
    df["Date"] = pd.to_datetime(df["Date"] + f" {year}", format="%d-%b %Y", errors="coerce")
    df = df.dropna(subset=["Date"]) # Drop rows where date conversion failed

    df["Min Temp"] = pd.to_numeric(df["Min Temp"], errors="coerce")
    df["Max Temp"] = pd.to_numeric(df["Max Temp"], errors="coerce")
    df["Avg Temp"] = (df["Min Temp"] + df["Max Temp"]) / 2
    df["CDD"] = (df["Avg Temp"] - 65).clip(lower=0).round(1)
    df["HDD"] = (65 - df["Avg Temp"]).clip(lower=0).round(1)
    df["Month-Day"] = df["Date"].dt.strftime("%m-%d")
    return df[["Date", "Min Temp", "Max Temp", "Avg Temp", "CDD", "HDD", "Month-Day"]]

def fetch_all_years(station_id, city_name, start_year=2005, end_year_override=None):
    """Fetches weather data for a range of years."""
    current_year = datetime.today().year
    end_year = end_year_override if end_year_override else current_year
    
    all_data = []
    for year in range(start_year, end_year + 1):
        print(f"Fetching weather data for {city_name}, year {year}...")
        year_df = fetch_year_data(station_id, city_name, year)
        if not year_df.empty:
            all_data.append(year_df)
    if not all_data:
        return pd.DataFrame()
    return pd.concat(all_data, ignore_index=True)

def compute_10yr_stats(df):
    """Computes 10-year historical statistics for weather data."""
    if df.empty or "Month-Day" not in df.columns or "Date" not in df.columns:
        print("⚠️ compute_10yr_stats: Input DataFrame is empty or missing required columns.")
        return df # Return original df if it's not suitable for processing

    # Ensure Date is datetime
    df["Date"] = pd.to_datetime(df["Date"])
    
    # Sort by date to ensure correct historical lookup
    df = df.sort_values(by="Date").reset_index(drop=True)

    output_rows = []
    for idx, row in df.iterrows():
        current_date = row["Date"]
        current_month_day = row["Month-Day"]
        
        # Look at the previous 10 years of data for the same month-day
        # Ensure we don't look into the future relative to the row's date
        historical_window_start = current_date - pd.DateOffset(years=10)
        
        same_day_historical = df[
            (df["Month-Day"] == current_month_day) &
            (df["Date"] < current_date) &
            (df["Date"] >= historical_window_start)
        ]
        
        stats = {
            "10yr Min Temp": np.nan, "10yr Max Temp": np.nan, "10yr Avg Temp": np.nan,
            "10yr CDD": np.nan, "10yr HDD": np.nan
        }

        if not same_day_historical.empty:
            tmin_10yr = same_day_historical["Min Temp"].min()
            tmax_10yr = same_day_historical["Max Temp"].max()
            
            # Calculate 10yr Avg Temp based on the average of 10yr Min and 10yr Max for that specific day type
            # Or, average the 'Avg Temp' of those historical days
            avg_temp_10yr_values = same_day_historical["Avg Temp"].dropna()
            tavg_10yr = avg_temp_10yr_values.mean() if not avg_temp_10yr_values.empty else np.nan

            if not np.isnan(tavg_10yr):
                stats["10yr Min Temp"] = round(tmin_10yr, 1)
                stats["10yr Max Temp"] = round(tmax_10yr, 1)
                stats["10yr Avg Temp"] = round(tavg_10yr, 1)
                stats["10yr CDD"] = round(max(0, tavg_10yr - 65), 1)
                stats["10yr HDD"] = round(max(0, 65 - tavg_10yr), 1)
        
        output_rows.append({**row.to_dict(), **stats})
        
    return pd.DataFrame(output_rows)

# --- LMP Data Function ---
def load_lmp_data(iso, location, start_date, end_date):
    """Loads LMP data from the appropriate CSV file."""
    file_path = ISO_LMP_FILES.get(iso)
    if not file_path or not file_path.exists():
        print(f"❌ LMP file not found for ISO {iso} at {file_path}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(file_path)
        df.columns = df.columns.str.strip().str.lower() # Standardize column names
        
        # Ensure 'date' and 'location' columns exist
        if 'date' not in df.columns:
            raise KeyError("LMP data CSV must contain a 'date' column.")
        if 'location' not in df.columns:
            # Try to find a location-like column, e.g., 'name', 'node'
            loc_col_candidates = [col for col in ['name', 'node', 'lmp_node', 'location name'] if col in df.columns]
            if not loc_col_candidates:
                 raise KeyError("LMP data CSV must contain a 'location' column (or similar like 'name', 'node').")
            df.rename(columns={loc_col_candidates[0]: 'location'}, inplace=True)


        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df["location"] = df["location"].astype(str).str.strip().str.upper() # Ensure string type and standardize
        
        # Filter by location and date
        # Ensure start_date and end_date are datetime objects for comparison
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        filtered_df = df[
            (df["location"] == location.upper()) &
            (df["date"] >= start_dt) &
            (df["date"] <= end_dt)
        ]
        return filtered_df
    except Exception as e:
        print(f"❌ Error loading LMP data for {iso} from {file_path}: {e}")
        return pd.DataFrame()

# --- Main Execution ---
def main():
    print("Fetching WSI City ID map...")
    try:
        city_map_wsi = get_city_map()
    except Exception as e:
        print(f"CRITICAL ERROR: Could not fetch WSI city map. {e}")
        return

    # --- User Selections ---
    # 1. Choose ISO
    available_isos = sorted(list(set(city_location_df["Iso"].dropna())))
    if not available_isos:
        print("❌ No ISOs found in the mapping file.")
        return
    iso_choice = questionary.select("🔌 Choose an ISO:", choices=available_isos).ask()
    if not iso_choice: return

    # 2. Choose Location within ISO
    locations_in_iso = sorted(list(set(
        city_location_df[city_location_df["Iso"] == iso_choice]["Location"].dropna()
    )))
    if not locations_in_iso:
        print(f"❌ No locations found for ISO {iso_choice} in the mapping file.")
        return
    location_choice = questionary.select(f"📍 Choose a location in {iso_choice}:", choices=locations_in_iso).ask()
    if not location_choice: return

    # 3. Choose City within Location
    cities_in_location = sorted(list(set(
        city_location_df[
            (city_location_df["Iso"] == iso_choice) &
            (city_location_df["Location"] == location_choice)
        ]["City"].dropna()
    )))
    if not cities_in_location:
        print(f"❌ No cities found for location {location_choice} in ISO {iso_choice}.")
        return
    city_choice = questionary.select(f"🏙️ Choose a city at {location_choice} ({iso_choice}):", choices=cities_in_location).ask()
    if not city_choice: return

    # 4. Choose Dates
    while True:
        start_str = questionary.text(
            "🗓️ Enter start date (YYYY-MM-DD):",
            default=datetime(datetime.today().year - 1, 1, 1).strftime("%Y-%m-%d")
        ).ask()
        if not start_str: return
        try:
            start_date = pd.to_datetime(start_str)
            break
        except ValueError:
            print("❌ Invalid start date format. Please use YYYY-MM-DD.")

    while True:
        end_str = questionary.text(
            "🗓️ Enter end date (YYYY-MM-DD):",
            default=datetime.today().strftime("%Y-%m-%d")
        ).ask()
        if not end_str: return
        try:
            end_date = pd.to_datetime(end_str)
            if end_date < start_date:
                print("❌ End date cannot be before start date.")
                continue
            break
        except ValueError:
            print("❌ Invalid end date format. Please use YYYY-MM-DD.")

    # --- Fetch Data ---
    station_id = city_map_wsi.get(city_choice)
    if not station_id:
        # Try to find a match ignoring case or common suffixes like 'Intl Airport'
        for wsi_city_name, s_id in city_map_wsi.items():
            if city_choice.lower() in wsi_city_name.lower():
                station_id = s_id
                print(f"ℹ️ Matched '{city_choice}' to WSI station '{wsi_city_name}' (ID: {station_id})")
                break
        if not station_id:
            print(f"❌ Station ID not found for city '{city_choice}' in WSI map.")
            print(f"   Available WSI station names near your choice: {[name for name in city_map_wsi.keys() if city_choice.split()[0].lower() in name.lower()][:10]}")
            return

    print(f"\nFetching weather data for {city_choice} (Station ID: {station_id}). This may take a few minutes...")
    # Fetch data from 2005 up to the end_date's year for 10yr stats calculation
    # Max year for fetching historical data should be end_date.year
    # Start year for 10yr averages can be fixed or dynamic (e.g., start_date.year - 10)
    weather_fetch_start_year = 2005 # Or adjust as needed for 10-year history base
    weather_df = fetch_all_years(station_id, city_choice, start_year=weather_fetch_start_year, end_year_override=end_date.year)

    if weather_df.empty:
        print(f"❌ No weather data found for {city_choice} between {weather_fetch_start_year} and {end_date.year}.")
        return

    print("Computing 10-year historical weather statistics...")
    weather_df = compute_10yr_stats(weather_df)
    
    # Filter weather data to the user-selected date range
    weather_df_filtered = weather_df[weather_df["Date"].between(start_date, end_date, inclusive="both")].copy() # Use .copy() to avoid SettingWithCopyWarning
    
    if weather_df_filtered.empty:
        print(f"❌ No weather data available for {city_choice} in the selected period: {start_str} to {end_str}.")
        return

    weather_df_filtered["Year"] = weather_df_filtered["Date"].dt.year

    print(f"\nLoading LMP data for {location_choice} ({iso_choice})...")
    lmp_df = load_lmp_data(iso_choice, location_choice, start_date, end_date)

    if lmp_df.empty:
        print(f"❌ No LMP data found for location {location_choice} ({iso_choice}) in the selected period.")
        # Proceed to show weather-only if user wants? For now, we exit.
        return
    
    # Ensure LMP data has a 'max_lmp' column or similar.
    # Common LMP column names: 'lmp', 'marginal_price_lmp', 'total_lmp', 'max_lmp'
    lmp_price_col_candidates = ['max_lmp', 'lmp', 'total_lmp', 'marginal_price_lmp', 'price']
    lmp_price_col = None
    for col in lmp_price_col_candidates:
        if col in lmp_df.columns:
            lmp_price_col = col
            break
    if not lmp_price_col:
        print(f"❌ Could not find a suitable LMP price column in {ISO_LMP_FILES.get(iso_choice)}. Expected one of {lmp_price_col_candidates}.")
        print(f"   Available columns in LMP data: {lmp_df.columns.tolist()}")
        return
    if lmp_price_col != 'max_lmp': # Standardize to 'max_lmp' for plotting
        lmp_df.rename(columns={lmp_price_col: 'max_lmp'}, inplace=True)


    print("\nMerging weather and LMP data...")
    merged_df = pd.merge(weather_df_filtered, lmp_df, left_on="Date", right_on="date", how="inner")

    if merged_df.empty:
        print("❌ No matching data found after merging weather and LMP for the selected period and location.")
        print(f"   Weather data points: {len(weather_df_filtered)}, LMP data points: {len(lmp_df)}")
        print(f"   Date range weather: {weather_df_filtered['Date'].min()} to {weather_df_filtered['Date'].max()}")
        print(f"   Date range LMP: {lmp_df['date'].min()} to {lmp_df['date'].max()}")
        return

    merged_df["Year"] = merged_df["Date"].dt.year # Ensure 'Year' is from the primary 'Date' column

    # --- Plotting ---
    print("Generating plot...")
    plt.figure(figsize=(14, 8))
    
    # Use a consistent color map if number of years is large, otherwise default
    num_years = merged_df["Year"].nunique()
    if num_years <= 10:
        colors = plt.cm.get_cmap('tab10', num_years).colors
    elif num_years <=20:
        colors = plt.cm.get_cmap('tab20', num_years).colors
    else: # For many years, 'viridis' or other continuous maps might be better, but need discrete colors
        colors = [plt.cm.get_cmap('viridis')(i/num_years) for i in range(num_years)]


    years = sorted(merged_df["Year"].unique())

    for i, year in enumerate(years):
        year_data = merged_df[merged_df["Year"] == year]
        if year_data.empty or year_data["Avg Temp"].isnull().all() or year_data["max_lmp"].isnull().all():
            continue

        color = colors[i % len(colors)]
        plt.scatter(year_data["Avg Temp"], year_data["max_lmp"], color=color, label=f"{year}", alpha=0.6, s=50)
        
        # Calculate line of best fit if there are enough data points
        valid_points = year_data.dropna(subset=["Avg Temp", "max_lmp"])
        if len(valid_points) >= 2:
            try:
                z = np.polyfit(valid_points["Avg Temp"], valid_points["max_lmp"], 1) # Degree 1 polynomial (line)
                p = np.poly1d(z)
                x_vals = np.linspace(valid_points["Avg Temp"].min(), valid_points["Avg Temp"].max(), 100)
                plt.plot(x_vals, p(x_vals), color=color, linestyle="--", linewidth=2)
            except (np.linalg.LinAlgError, TypeError) as e:
                print(f"Could not compute trendline for {year}: {e}")


    plt.title(f"Daily Average Temperature vs. Max LMP for {city_choice} ({location_choice}, {iso_choice})\n({start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')})", fontsize=16)
    plt.xlabel("Average Temperature (°F)", fontsize=14)
    plt.ylabel(f"Max LMP ({lmp_price_col.replace('_',' ').title()} $/MWh)", fontsize=14)
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.legend(title="Year", bbox_to_anchor=(1.03, 1), loc='upper left')
    plt.tight_layout(rect=[0, 0, 0.88, 1]) # Adjust layout to make space for legend
    
    print("\nDisplaying plot. Close the plot window to exit.")
    plt.show()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\nPress Enter to exit...")