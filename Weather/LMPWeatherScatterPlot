from dotenv import load_dotenv
import os
import requests
import pandas as pd
import matplotlib
matplotlib.use('TkAgg') # Explicitly set backend for wider compatibility
import matplotlib.pyplot as plt
from io import StringIO
from datetime import datetime
from pathlib import Path
import questionary
import numpy as np

# --- Define Base Path (assuming the script is in trader-helper/Weather/ or similar) ---
# If this script is in 'trader-helper/Weather/', then base_dir is 'trader-helper'
try:
    # Assuming the script is one level down from the base_dir (e.g., in a 'scripts' or 'Weather' folder)
    BASE_DIR = Path(__file__).resolve().parents[1]
except NameError:
    # Fallback if __file__ is not defined (e.g., running in an interactive environment)
    # You might need to adjust this if your script is located elsewhere relative to 'trader-helper'
    BASE_DIR = Path(".").resolve() # Assumes script is run from 'trader-helper' or 'trader-helper/Weather'
    if not (BASE_DIR / "INFO").exists() and (BASE_DIR.parent / "INFO").exists():
        BASE_DIR = BASE_DIR.parent # Adjust if script is in 'Weather' and run from 'Weather'

ENV_PATH = BASE_DIR / "Weather" / ".env"
INFO_DIR = BASE_DIR / "INFO"

# --- Load credentials from .env ---
load_dotenv(dotenv_path=ENV_PATH)
USERNAME = os.getenv("WSI_USERNAME")
PROFILE = os.getenv("WSI_PROFILE")
PASSWORD = os.getenv("WSI_PASSWORD")

if not USERNAME or not PROFILE or not PASSWORD:
    print(f"Attempted to load .env from: {ENV_PATH}")
    raise EnvironmentError("❌ Missing WSI credentials. Ensure .env exists at trader-helper/Weather/.env and is correctly formatted.")

# --- Load city to ISO and location mapping ---
mapping_file = INFO_DIR / "City_to_ISO_and_Location_Mapping.csv"
if not mapping_file.exists():
    raise FileNotFoundError(f"❌ Mapping file not found: {mapping_file}")
city_location_df = pd.read_csv(mapping_file)
city_location_df.columns = city_location_df.columns.str.strip().str.title()
CITY_TO_ISO = dict(zip(city_location_df["City"], city_location_df["Iso"]))
CITY_TO_LOCATION = dict(zip(city_location_df["City"], city_location_df["Location"]))

# --- ISO to file mappings ---
ISO_LMP_FILES = {
    "CAISO": INFO_DIR / "CAISOMaxLMP.csv",
    "ERCOT": INFO_DIR / "ERCOTMaxLMP.csv",
    "PJM": INFO_DIR / "PJMMaxLMP.csv",
    "MISO": INFO_DIR / "MISOMaxLMP.csv",
    "SPP": INFO_DIR / "SPPMaxLMP.csv",
    "NYISO": INFO_DIR / "NYISOMaxLMP.csv",
    "ISO-NE": INFO_DIR / "ISONEMaxLMP.csv"
}

# --- WSI API Functions ---
def get_city_map():
    """Fetches the city to SiteId mapping from WSI."""
    url = "https://www.wsitrader.com/Services/CSVDownloadService.svc/GetCityIds"
    params = {"Account": USERNAME, "Profile": PROFILE, "Password": PASSWORD}
    try:
        response = requests.get(url, params=params, timeout=20)
        response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)
    except requests.exceptions.RequestException as e:
        raise ConnectionError(f"Failed to connect to WSI API (GetCityIds): {e}")

    df = pd.read_csv(StringIO(response.text), sep=",", engine="python", on_bad_lines='skip')
    df.columns = df.columns.str.strip()
    if not {"Station Name", "SiteId"}.issubset(df.columns):
        raise KeyError(f"Expected columns 'Station Name', 'SiteId' not found in WSI city map. Columns are: {df.columns.tolist()}")
    return dict(zip(df["Station Name"], df["SiteId"]))

def fetch_year_data(station_id, city_name, year):
    """Fetches historical daily average temperature data for a given year from WSI."""
    def fmt(d): return d.strftime("%m/%d/%Y")
    url = "https://www.wsitrader.com/Services/CSVDownloadService.svc/GetHistoricalObservations"
    params = {
        "Account": USERNAME, "Profile": PROFILE, "Password": PASSWORD,
        "CityIds[]": station_id, "StartDate": fmt(datetime(year, 1, 1)), "EndDate": fmt(datetime(year, 12, 31)),
        "HistoricalProductId": "HISTORICAL_DAILY_AVERAGE", "DataTypes[]": "temperature",
        "TempUnits": "F", "IsTemp": "true", "IsDaily": "true", "IsDisplayDates": "false"
    }
    try:
        response = requests.get(url, params=params, timeout=60) # Increased timeout for potentially large data
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"⚠️ Warning: Failed to fetch data for {city_name} year {year} from WSI: {e}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(StringIO(response.text), sep=",", engine="python", on_bad_lines='skip')
    except pd.errors.EmptyDataError:
        print(f"ℹ️ No data returned for {city_name} year {year}.")
        return pd.DataFrame()

    if df.empty or len(df.columns) < 3:
        print(f"ℹ️ Empty or malformed data for {city_name} year {year}. Columns: {df.columns.tolist() if not df.empty else 'N/A'}")
        return pd.DataFrame()

    df.columns = df.columns.str.strip()
    # Ensure the renaming is robust to potential slight column name variations
    # The first column is Date, next two are temperature readings.
    date_col_name = df.columns[0]
    min_temp_col_name = df.columns[1]
    max_temp_col_name = df.columns[2]

    df = df.rename(columns={date_col_name: "Date", min_temp_col_name: "Min Temp", max_temp_col_name: "Max Temp"})
    df["Date"] = pd.to_datetime(df["Date"] + f" {year}", format="%d-%b %Y", errors="coerce")
    df = df.dropna(subset=["Date"]) # Drop rows where date conversion failed

    df["Min Temp"] = pd.to_numeric(df["Min Temp"], errors="coerce")
    df["Max Temp"] = pd.to_numeric(df["Max Temp"], errors="coerce")
    df["Avg Temp"] = (df["Min Temp"] + df["Max Temp"]) / 2
    df["CDD"] = (df["Avg Temp"] - 65).clip(lower=0).round(1)
    df["HDD"] = (65 - df["Avg Temp"]).clip(lower=0).round(1)
    df["Month-Day"] = df["Date"].dt.strftime("%m-%d")
    return df[["Date", "Min Temp", "Max Temp", "Avg Temp", "CDD", "HDD", "Month-Day"]]

def fetch_all_years(station_id, city_name, start_year=2005, end_year_override=None):
    """Fetches weather data for a range of years."""
    current_year = datetime.today().year
    end_year = end_year_override if end_year_override else current_year
    
    all_data = []
    for year_idx in range(start_year, end_year + 1):
        print(f"Fetching weather data for {city_name}, year {year_idx}...")
        year_df = fetch_year_data(station_id, city_name, year_idx)
        if not year_df.empty:
            all_data.append(year_df)
    if not all_data:
        return pd.DataFrame()
    return pd.concat(all_data, ignore_index=True)

def compute_10yr_stats(df):
    """Computes 10-year historical statistics for weather data."""
    if df.empty or "Month-Day" not in df.columns or "Date" not in df.columns:
        print("⚠️ compute_10yr_stats: Input DataFrame is empty or missing required columns.")
        return df # Return original df if it's not suitable for processing

    # Ensure Date is datetime
    df["Date"] = pd.to_datetime(df["Date"])
    
    # Sort by date to ensure correct historical lookup
    df = df.sort_values(by="Date").reset_index(drop=True)

    output_rows = []
    for idx, row in df.iterrows():
        current_date = row["Date"]
        current_month_day = row["Month-Day"]
        
        # Look at the previous 10 years of data for the same month-day
        # Ensure we don't look into the future relative to the row's date
        historical_window_start = current_date - pd.DateOffset(years=10)
        
        same_day_historical = df[
            (df["Month-Day"] == current_month_day) &
            (df["Date"] < current_date) &
            (df["Date"] >= historical_window_start)
        ]
        
        stats = {
            "10yr Min Temp": np.nan, "10yr Max Temp": np.nan, "10yr Avg Temp": np.nan,
            "10yr CDD": np.nan, "10yr HDD": np.nan
        }

        if not same_day_historical.empty:
            tmin_10yr = same_day_historical["Min Temp"].min()
            tmax_10yr = same_day_historical["Max Temp"].max()
            
            avg_temp_10yr_values = same_day_historical["Avg Temp"].dropna()
            tavg_10yr = avg_temp_10yr_values.mean() if not avg_temp_10yr_values.empty else np.nan

            if not np.isnan(tavg_10yr):
                stats["10yr Min Temp"] = round(tmin_10yr, 1)
                stats["10yr Max Temp"] = round(tmax_10yr, 1)
                stats["10yr Avg Temp"] = round(tavg_10yr, 1)
                stats["10yr CDD"] = round(max(0, tavg_10yr - 65), 1)
                stats["10yr HDD"] = round(max(0, 65 - tavg_10yr), 1)
        
        output_rows.append({**row.to_dict(), **stats})
        
    return pd.DataFrame(output_rows)

# --- LMP Data Function (UPDATED) ---
def load_lmp_data(iso, location, start_date, end_date):
    """Loads LMP data from the appropriate CSV file."""
    file_path = ISO_LMP_FILES.get(iso)
    if not file_path or not file_path.exists():
        print(f"❌ LMP file not found for ISO {iso} at {file_path}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(file_path)
        original_columns = df.columns.tolist() # Keep original for messages
        df.columns = df.columns.str.strip() # Basic strip first

        # --- Robust Date Column Detection ---
        date_col_in_df = None
        date_col_candidates = ['date', 'datetime', 'day', 'time', 'period']
        for candidate in date_col_candidates:
            for col_in_df_loop in df.columns:
                if candidate == col_in_df_loop.lower().replace('_', ''):
                    date_col_in_df = col_in_df_loop
                    break
            if date_col_in_df:
                break
        if not date_col_in_df:
            raise KeyError(f"LMP data CSV {file_path} must contain a 'date' column or similar (e.g., {', '.join(date_col_candidates)}). Found: {original_columns}")
        if date_col_in_df != 'date': # Standardize name
            df.rename(columns={date_col_in_df: 'date'}, inplace=True)

        # --- Robust Location Column Detection ---
        loc_col_in_df = None
        # Prioritize 'location' if it exists exactly, ignoring case for initial check
        for col_in_df_loop in df.columns:
            if 'location' == col_in_df_loop.lower():
                loc_col_in_df = col_in_df_loop
                break
        
        if not loc_col_in_df: # If 'location' not found, try other candidates
            loc_col_candidates = ['node', 'name', 'pnode', 'settlementpointname', 'settlement_point_name', 'lmp_node', 'location name']
            for candidate in loc_col_candidates:
                for col_in_df_loop in df.columns:
                    # Normalize both candidate and column name for matching
                    norm_col = col_in_df_loop.lower().replace('_', '').replace(' ', '')
                    norm_cand = candidate.lower().replace('_', '').replace(' ', '')
                    if norm_cand == norm_col:
                        loc_col_in_df = col_in_df_loop
                        break
                if loc_col_in_df:
                    break
        
        if not loc_col_in_df:
            raise KeyError(f"LMP data CSV {file_path} must contain a 'location' column or similar. Tried 'location' and candidates like 'node', 'name', etc. Found: {original_columns}")
        if loc_col_in_df != 'location': # Standardize name
            df.rename(columns={loc_col_in_df: 'location'}, inplace=True)


        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df["location"] = df["location"].astype(str).str.strip().str.upper()

        # --- Improved LMP Price Column Detection ---
        lmp_price_col = None # This will store the original name of the identified LMP price column
        simplified_to_original_cols = {} # Maps simplified name to original name

        for col_original_name in df.columns:
            # Skip already identified/standardized columns
            if col_original_name.lower() in ['date', 'location']:
                continue
            
            simplified = col_original_name.lower()
            simplified = simplified.split('(')[0].strip()  # Remove units like ($/mwh)
            simplified = simplified.replace('_', ' ').replace('-', ' ') # Normalize separators
            simplified = " ".join(simplified.split()) # Normalize multiple spaces to one
            simplified_to_original_cols[simplified] = col_original_name
        
        # Define candidates (order can imply priority)
        primary_keywords = ['max lmp', 'maxlmp'] # Highest priority
        secondary_keywords = ['lmp', 'price', 'total lmp', 'marginal price lmp'] # General terms

        # Search using primary keywords
        for keyword in primary_keywords:
            normalized_keyword = " ".join(keyword.split()) # Ensure keyword is also normalized
            for simplified_col_name, original_col_name in simplified_to_original_cols.items():
                if normalized_keyword in simplified_col_name: # Check if keyword is part of the simplified column name
                    lmp_price_col = original_col_name
                    break
            if lmp_price_col:
                break
        
        # If not found, search using secondary keywords
        if not lmp_price_col:
            for keyword in secondary_keywords:
                normalized_keyword = " ".join(keyword.split())
                for simplified_col_name, original_col_name in simplified_to_original_cols.items():
                    if normalized_keyword in simplified_col_name:
                        lmp_price_col = original_col_name
                        break
                if lmp_price_col: # Found one
                    break
        
        if not lmp_price_col:
            print(f"❌ Could not find a suitable LMP price column in {file_path}.")
            print(f"   Searched for keywords like: {primary_keywords + secondary_keywords}")
            print(f"   Available original columns in LMP data: {original_columns}")
            print(f"   Simplified column names processed for matching: {list(simplified_to_original_cols.keys())}")
            return pd.DataFrame()
        
        # Rename the found LMP price column to 'max_lmp' for standardization
        # This line is crucial for the rest of the script (e.g. plotting)
        if lmp_price_col != 'max_lmp':
            df.rename(columns={lmp_price_col: 'max_lmp'}, inplace=True)
        # --- End of Improved LMP Price Column Detection ---

        # Filter by location and date
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        # Now 'max_lmp' should exist in df.columns if renaming occurred
        if 'max_lmp' not in df.columns:
            # This should not happen if logic above is correct, but as a safeguard:
            print(f"Error: 'max_lmp' column not found after attempting to identify and rename LMP price column. Original price column: {lmp_price_col}")
            return pd.DataFrame()

        filtered_df = df[
            (df["location"] == location.upper()) &
            (df["date"] >= start_dt) &
            (df["date"] <= end_dt)
        ]
        return filtered_df
    except Exception as e:
        print(f"❌ Error loading LMP data for {iso} from {file_path}: {e}")
        import traceback
        traceback.print_exc() 
        return pd.DataFrame()

# --- Main Execution ---
def main():
    print("Fetching WSI City ID map...")
    try:
        city_map_wsi = get_city_map()
    except Exception as e:
        print(f"CRITICAL ERROR: Could not fetch WSI city map. {e}")
        return

    # --- User Selections ---
    available_isos = sorted(list(set(city_location_df["Iso"].dropna().astype(str).str.upper())))
    if not available_isos:
        print("❌ No ISOs found in the mapping file.")
        return
    iso_choice_q = questionary.select("🔌 Choose an ISO:", choices=available_isos).ask()
    if not iso_choice_q: return
    iso_choice = iso_choice_q.upper() # Standardize ISO choice

    locations_in_iso = sorted(list(set(
        city_location_df[city_location_df["Iso"].astype(str).str.upper() == iso_choice]["Location"].dropna().astype(str).str.upper()
    )))
    if not locations_in_iso:
        print(f"❌ No locations found for ISO {iso_choice} in the mapping file.")
        return
    location_choice_q = questionary.select(f"📍 Choose a location in {iso_choice}:", choices=locations_in_iso).ask()
    if not location_choice_q: return
    location_choice = location_choice_q.upper() # Standardize location choice

    cities_in_location = sorted(list(set(
        city_location_df[
            (city_location_df["Iso"].astype(str).str.upper() == iso_choice) &
            (city_location_df["Location"].astype(str).str.upper() == location_choice)
        ]["City"].dropna()
    )))
    if not cities_in_location:
        print(f"❌ No cities found for location {location_choice} in ISO {iso_choice}.")
        return
    city_choice = questionary.select(f"🏙️ Choose a city at {location_choice} ({iso_choice}):", choices=cities_in_location).ask()
    if not city_choice: return

    while True:
        start_str = questionary.text(
            "🗓️ Enter start date (YYYY-MM-DD):",
            default=datetime(datetime.today().year - 1, 1, 1).strftime("%Y-%m-%d")
        ).ask()
        if not start_str: return
        try:
            start_date = pd.to_datetime(start_str)
            break
        except ValueError:
            print("❌ Invalid start date format. Please use YYYY-MM-DD.")

    while True:
        end_str = questionary.text(
            "🗓️ Enter end date (YYYY-MM-DD):",
            default=datetime.today().strftime("%Y-%m-%d")
        ).ask()
        if not end_str: return
        try:
            end_date = pd.to_datetime(end_str)
            if end_date < start_date:
                print("❌ End date cannot be before start date.")
                continue
            break
        except ValueError:
            print("❌ Invalid end date format. Please use YYYY-MM-DD.")

    station_id = city_map_wsi.get(city_choice)
    if not station_id:
        for wsi_city_name, s_id in city_map_wsi.items():
            if city_choice.lower() in wsi_city_name.lower():
                station_id = s_id
                print(f"ℹ️ Matched '{city_choice}' to WSI station '{wsi_city_name}' (ID: {station_id})")
                break
        if not station_id:
            print(f"❌ Station ID not found for city '{city_choice}' in WSI map.")
            print(f"   Available WSI station names near your choice: {[name for name in city_map_wsi.keys() if city_choice.split()[0].lower() in name.lower()][:10]}")
            return

    print(f"\nFetching weather data for {city_choice} (Station ID: {station_id}). This may take a few minutes...")
    weather_fetch_start_year = 2005 
    weather_df = fetch_all_years(station_id, city_choice, start_year=weather_fetch_start_year, end_year_override=end_date.year)

    if weather_df.empty:
        print(f"❌ No weather data found for {city_choice} between {weather_fetch_start_year} and {end_date.year}.")
        return

    print("Computing 10-year historical weather statistics...")
    weather_df = compute_10yr_stats(weather_df)
    
    weather_df_filtered = weather_df[weather_df["Date"].between(start_date, end_date, inclusive="both")].copy()
    
    if weather_df_filtered.empty:
        print(f"❌ No weather data available for {city_choice} in the selected period: {start_str} to {end_str}.")
        return
    weather_df_filtered["Year"] = weather_df_filtered["Date"].dt.year

    print(f"\nLoading LMP data for {location_choice} ({iso_choice})...")
    # Pass the already upper-cased iso_choice and location_choice
    lmp_df = load_lmp_data(iso_choice, location_choice, start_date, end_date)

    if lmp_df.empty:
        print(f"❌ No LMP data found for location {location_choice} ({iso_choice}) in the selected period.")
        return
    
    # After load_lmp_data, the LMP price column should be named 'max_lmp'
    if 'max_lmp' not in lmp_df.columns:
        print(f"❌ Critical Error: 'max_lmp' column is missing from LMP data after processing. Columns: {lmp_df.columns.tolist()}")
        return
    
    # The variable 'lmp_price_col' previously used for the y-axis label is no longer defined in this scope.
    # We know the column is 'max_lmp', so we can use that directly or retrieve the original name if needed for display.
    # For simplicity in label, we can assume it's "Max LMP" or derive from the known standardized name.
    y_axis_label_price_col_name = "Max LMP" 


    print("\nMerging weather and LMP data...")
    # Ensure 'date' column in lmp_df is consistently named for the merge
    if 'date' not in lmp_df.columns and 'Date' in lmp_df.columns: # if 'date' was somehow changed to 'Date'
        lmp_df.rename(columns={'Date':'date'}, inplace=True)
    elif 'date' not in lmp_df.columns:
        print(f"❌ LMP DataFrame does not have a 'date' column for merging. Columns: {lmp_df.columns.tolist()}")
        return

    merged_df = pd.merge(weather_df_filtered, lmp_df, left_on="Date", right_on="date", how="inner")

    if merged_df.empty:
        print("❌ No matching data found after merging weather and LMP for the selected period and location.")
        print(f"   Weather data points: {len(weather_df_filtered)}, LMP data points: {len(lmp_df)}")
        if not weather_df_filtered.empty:
             print(f"   Date range weather: {weather_df_filtered['Date'].min()} to {weather_df_filtered['Date'].max()}")
        if not lmp_df.empty:
             print(f"   Date range LMP: {lmp_df['date'].min()} to {lmp_df['date'].max()}")
        return

    merged_df["Year"] = merged_df["Date"].dt.year 

    print("Generating plot...")
    plt.figure(figsize=(14, 8))
    
    num_years = merged_df["Year"].nunique()
    cmap_name = 'viridis' # Default continuous map
    if num_years <= 10: cmap_name = 'tab10'
    elif num_years <=20: cmap_name = 'tab20'
    
    # Get a color for each unique year
    unique_years_sorted = sorted(merged_df["Year"].unique())
    year_colors = {year_val: plt.cm.get_cmap(cmap_name)(i / max(1, num_years -1 if num_years > 1 else 1)) 
                   for i, year_val in enumerate(unique_years_sorted)}


    for year_val_loop in unique_years_sorted:
        year_data = merged_df[merged_df["Year"] == year_val_loop]
        if year_data.empty or year_data["Avg Temp"].isnull().all() or year_data["max_lmp"].isnull().all():
            continue

        color = year_colors[year_val_loop]
        plt.scatter(year_data["Avg Temp"], year_data["max_lmp"], color=color, label=f"{year_val_loop}", alpha=0.6, s=50)
        
        valid_points = year_data.dropna(subset=["Avg Temp", "max_lmp"])
        if len(valid_points) >= 2:
            try:
                z = np.polyfit(valid_points["Avg Temp"], valid_points["max_lmp"], 1) 
                p = np.poly1d(z)
                min_temp_val = valid_points["Avg Temp"].min()
                max_temp_val = valid_points["Avg Temp"].max()
                if min_temp_val == max_temp_val: # Avoids issue with linspace if only one unique x value
                    x_vals = np.array([min_temp_val, max_temp_val])
                else:
                    x_vals = np.linspace(min_temp_val, max_temp_val, 100)
                plt.plot(x_vals, p(x_vals), color=color, linestyle="--", linewidth=2)
            except (np.linalg.LinAlgError, TypeError) as e:
                print(f"Could not compute trendline for {year_val_loop}: {e}")

    plt.title(f"Daily Average Temperature vs. Max LMP for {city_choice} ({location_choice}, {iso_choice})\n({start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')})", fontsize=16)
    plt.xlabel("Average Temperature (°F)", fontsize=14)
    plt.ylabel(f"{y_axis_label_price_col_name} ($/MWh)", fontsize=14) # Use the determined label
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.legend(title="Year", bbox_to_anchor=(1.03, 1), loc='upper left')
    plt.tight_layout(rect=[0, 0, 0.88, 1]) 
    
    print("\nDisplaying plot. Close the plot window to exit.")
    plt.show()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\nPress Enter to exit...")
