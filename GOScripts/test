import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.metrics import mean_absolute_error
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
import json

# --- Data Loading ---
data_folder_name = "INFO"

# Load primary datasets
try:
    eia_changes = pd.read_csv(f"{data_folder_name}/EIAchanges.csv", parse_dates=["Period"]).set_index("Period")
    eia_totals = pd.read_csv(f"{data_folder_name}/EIAtotals.csv", parse_dates=["Period"]).set_index("Period")
    weather = pd.read_csv(f"{data_folder_name}/WEATHER.csv", parse_dates=["Date"])
    fundy = pd.read_csv(f"{data_folder_name}/Fundy.csv", parse_dates=["Date"])
    criterion_df = pd.read_csv(f"{data_folder_name}/CriterionStorageChange.csv", parse_dates=["eff_gas_day"])
    locs_df = pd.read_csv(f"{data_folder_name}/locs_list.csv").drop_duplicates(subset="storage_name")
except FileNotFoundError as e:
    print(f"Error loading file: {e}. Please ensure all CSV files are uploaded and located in the '{data_folder_name}' folder.")
    exit()
except Exception as e:
    print(f"Data loading error: {e}")
    exit()

# === Region-specific feature toggles ===
REGION_FEATURE_TOGGLES = {
    'Lower 48 States': {'use_criterion': False, 'enhance_weather': True, 'model_type': 'Lasso'},
    'East Region': {'use_criterion': False, 'enhance_weather': False, 'model_type': 'Ridge'},
    'Midwest Region': {'use_criterion': True, 'enhance_weather': True, 'model_type': 'Ridge'},
    'South Central Region': {'use_criterion': False, 'enhance_weather': False, 'model_type': 'Lasso'},
    'Mountain Region': {'use_criterion': False, 'enhance_weather': True, 'model_type': 'Lasso'},
    'Pacific Region': {'use_criterion': True, 'enhance_weather': False, 'model_type': 'Ridge'}
}
# === END Region-specific feature toggles ===

# --- State to Region Mapping ---
state_to_region = {
    "Maine": "East Region", "New Hampshire": "East Region", "New York": "East Region", "New Jersey": "East Region",
    "Massachusetts": "East Region", "Rhode Island": "East Region", "Connecticut": "East Region", "Pennsylvania": "East Region",
    "Ohio": "East Region", "West Virginia": "East Region", "Delaware": "East Region", "Maryland": "East Region",
    "Virginia": "East Region", "North Carolina": "East Region", "South Carolina": "East Region",
    "Georgia": "East Region", "Florida": "East Region",
    "Illinois": "Midwest Region", "Indiana": "Midwest Region", "Michigan": "Midwest Region", "Wisconsin": "Midwest Region",
    "Minnesota": "Midwest Region", "Iowa": "Midwest Region", "Missouri": "Midwest Region", "North Dakota": "Midwest Region",
    "South Dakota": "South Dakota", "Nebraska": "Midwest Region", "Kansas": "Midwest Region",
    "Texas": "South Central Region", "Oklahoma": "South Central Region", "Louisiana": "South Central Region",
    "Arkansas": "South Central Region", "Mississippi": "South Central Region", "Alabama": "South Central Region",
    "Tennessee": "South Central Region", "Kentucky": "South Central Region",
    "Colorado": "Mountain Region", "Utah": "Mountain Region", "New Mexico": "New Mexico", "Arizona": "Mountain Region", # Corrected New Mexico
    "Montana": "Mountain Region", "Idaho": "Mountain Region", "Wyoming": "Mountain Region", "Nevada": "Mountain Region",
    "California": "Pacific Region", "Oregon": "Pacific Region", "Washington": "Pacific Region",
    "Alaska": "Pacific Region", "Hawaii": "Pacific Region"
}

# --- Preprocess Criterion Data ---
print("Preprocessing Criterion data...")
criterion_merged = criterion_df.merge(locs_df, on="storage_name", how="left")
criterion_merged = criterion_merged.dropna(subset=["state_name", "storage_name"])
criterion_merged["Region"] = criterion_merged["state_name"].map(state_to_region)
criterion_merged = criterion_merged.dropna(subset=["Region"])
criterion_merged.set_index("eff_gas_day", inplace=True)

criterion_weekly = (
    criterion_merged.groupby([pd.Grouper(freq="W-FRI"), "Region"])['daily_storage_change']
    .sum()
    .unstack("Region")
    .rename(columns=lambda r: f"CriterionChange_{r.replace(' ', '')}")
)
criterion_weekly.index.name = 'Period'
print("Criterion data preprocessed.")


# --- Region Mapping ---
REGION_COLUMNS = {
    "Lower 48 States": "Lower 48 States Storage Change (Bcf)",
    "East Region": "East Region Storage Change (Bcf)",
    "Midwest Region": "Midwest Region Storage Change (Bcf)",
    "South Central Region": "South Central Region Storage Change (Bcf)",
    "Mountain Region": "Mountain Region Storage Change (Bcf)",
    "Pacific Region": "Pacific Region Storage Change (Bcf)",
}

REGION_CITY = {
    "Lower 48 States": "Conus",
    "East Region": "Boston",
    "Midwest Region": "Chicago OHare",
    "South Central Region": "Houston IAH",
    "Mountain Region": "Denver",
    "Pacific Region": "Los Angeles",
}

# --- Preprocessing Fundy Data ---
print("Preprocessing Fundy data...")
fundy_pivoted = fundy.pivot_table(index='Date', columns='item', values='value')
fundy_weekly_resampled = fundy_pivoted.resample('W-FRI').sum()
fundy_weekly_resampled.index.name = 'Period'
print("Fundy data preprocessed.")

# --- Preprocessing Weather Data ---
print("Preprocessing Weather data...")
unique_weather_cities = weather['City Title'].unique()
print(f"Cities found in WEATHER.csv: {', '.join(unique_weather_cities)}")

weather_weekly_list = []
for city in unique_weather_cities:
    city_weather = weather[weather['City Title'] == city].copy()
    city_weather = city_weather.set_index('Date')
    
    weekly_data = city_weather[['Avg Temp', 'CDD', 'HDD', 'Daily Precip Amount']].resample('W-FRI').agg({
        'Avg Temp': 'mean',
        'CDD': 'sum',
        'HDD': 'sum',
        'Daily Precip Amount': 'sum'
    })
    weekly_data.columns = [f"{city}_{col.replace(' ', '')}" for col in weekly_data.columns]
    weather_weekly_list.append(weekly_data)

weather_weekly_df = pd.concat(weather_weekly_list, axis=1)
weather_weekly_df.index.name = 'Period'

print("Aggregating all city weather data for 'Lower 48 States' (Conus)...")
conus_weather = weather.set_index('Date')
conus_weather_weekly = conus_weather[['Avg Temp', 'CDD', 'HDD', 'Daily Precip Amount']].resample('W-FRI').agg({
    'Avg Temp': 'mean',
    'CDD': 'sum',
    'HDD': 'sum',
    'Daily Precip Amount': 'sum'
})
conus_weather_weekly.columns = [f"Conus_{col.replace(' ', '')}" for col in conus_weather_weekly.columns]
weather_weekly_df = weather_weekly_df.join(conus_weather_weekly, how='outer')
print("Weather data preprocessed.")

# --- Prepare output storage ---
results = []

# --- Main loop for each region ---
for region, col in REGION_COLUMNS.items():
    print(f"\n==== EIA STORAGE FORECAST + BACKTEST ====\nRegion: {region}")

    # Get region-specific toggles
    toggles = REGION_FEATURE_TOGGLES.get(region, {})
    use_criterion = toggles.get("use_criterion", False)
    enhance_weather = toggles.get("enhance_weather", False)
    model_type = toggles.get("model_type", "Lasso") # Default to Lasso

    print(f"    ▶️ Use Criterion? {use_criterion}")
    print(f"    ▶️ Enhance Weather? {enhance_weather}")
    print(f"    ▶️ Model Type: {model_type}")

    if col not in eia_changes.columns:
        print(f"❌ Skipping {region}, target column not found in EIAchanges.csv: {col}")
        continue

    df = pd.DataFrame()
    df["Target"] = eia_changes[col]

    total_col = col.replace("Storage Change", "Storage")
    if total_col in eia_totals.columns:
        df = df.join(eia_totals[[total_col]], how="left")
        df.rename(columns={total_col: "StorageTotal"}, inplace=True)
    else:
        print(f"⚠️ Missing total storage data for {region} ({total_col}). Skipping region.")
        continue

    fundy_region_items = []
    if region == "Lower 48 States":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('CONUS -')]
    elif region == "East Region":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('Northeast -')]
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('SouthEast -')])
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('SouthEast[Fl] -')])
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('SouthEast[Oth] -')])
    elif region == "Midwest Region":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('Midwest -')]
    elif region == "South Central Region":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('SouthCentral -')]
    elif region == "Mountain Region":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('Rockies -')]
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('Rockies[SW] -')])
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('Rockies[Up] -')])
    elif region == "Pacific Region":
        fundy_region_items = [item for item in fundy_weekly_resampled.columns if item.startswith('West -')]
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('West[CA] -')])
        fundy_region_items.extend([item for item in fundy_weekly_resampled.columns if item.startswith('West[PNW] -')])
    
    if fundy_region_items:
        df = df.join(fundy_weekly_resampled[fundy_region_items], how="left")
    else:
        print(f"⚠️ No matching Fundy data items found for {region}. Skipping Fundy features.")

    city_for_weather = REGION_CITY.get(region)
    weather_cols_prefix = f"{city_for_weather}_"

    weather_features_for_region = [col for col in weather_weekly_df.columns if col.startswith(weather_cols_prefix)]
    
    if not weather_features_for_region:
        print(f"❌ Skipping {region}, weather data not found for city/conus: {city_for_weather}")
        continue

    df = df.join(weather_weekly_df[weather_features_for_region], how="left")

    new_weather_cols = {col: col.replace(weather_cols_prefix, "") for col in weather_features_for_region}
    df.rename(columns=new_weather_cols, inplace=True)

    base_weather_cols = ["AvgTemp", "CDD", "HDD", "DailyPrecipAmount"]
    for temp_col in base_weather_cols:
        if temp_col in df.columns:
            df[f"{temp_col}_roll3"] = df[temp_col].rolling(3).mean()
        else:
            print(f"Warning: Base weather column '{temp_col}' not found for rolling feature creation in {region}.")

    # === Conditional Enhanced Weather Features ===
    if enhance_weather:
        print(f"🌦️ Enhancing weather features for {region}")
        for feature in ["CDD", "HDD"]:
            if feature in df.columns:
                df[f"{feature}_lag1"] = df[feature].shift(1)
                df[f"{feature}_delta5yr"] = df[feature] - df[feature].rolling(window=260, min_periods=1).mean()
            else:
                print(f"⚠️ Skipped enhanced weather for {feature} in {region} – not found.")
    else:
        print(f"🌤️ Using standard weather for {region}")

    # === Conditional Criterion Data Merge ===
    criterion_col_name = f"CriterionChange_{region.replace(' ', '')}"
    if use_criterion:
        try:
            if criterion_col_name in criterion_weekly.columns:
                print(f"✅ Adding Criterion data for {region} ({criterion_col_name})")
                df = df.join(criterion_weekly[[criterion_col_name]], how="left")
            else:
                print(f"⚠️ Criterion column '{criterion_col_name}' not found for {region} — skipping merge.")
        except Exception as e:
            print(f"❌ Error merging Criterion for {region}: {e}")
    else:
        print(f"⏩ Skipping Criterion data for {region} as per model tuning.")

    # --- Create a Custom Feature: Rolling Injection Size ---
    df["Target_roll3"] = df["Target"].rolling(window=3, min_periods=1).mean().shift(1) 

    initial_rows = len(df)
    df.dropna(inplace=True)
    if len(df) < initial_rows:
        print(f"Dropped {initial_rows - len(df)} rows due to NaN values for {region}.")

    latest_training_data_date = df.index.max()
    cutoff_date = latest_training_data_date - timedelta(days=5 * 365)

    df_train = df[df.index >= cutoff_date].copy()

    if len(df_train) < 10:
        print(f"⚠️ Not enough usable data for {region} after 5-year cutoff. Records: {len(df_train)}. Skipping region.")
        continue

    features = df_train.drop(columns="Target")
    target = df_train["Target"]

    if features.empty:
        print(f"❌ Skipping {region}, features DataFrame is empty after preprocessing.")
        continue
    
    features = features.loc[:, (features != features.iloc[0]).any()]
    if features.empty:
        print(f"❌ Skipping {region}, all features are constant after filtering. Cannot train model.")
        continue

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(features)
    X_scaled_df = pd.DataFrame(X_scaled, columns=features.columns, index=features.index)
    print(f"✅ Features standardized for {region}.")

    # --- Model Selection ---
    if model_type == "Lasso":
        model = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0], max_iter=10000, random_state=42)
    elif model_type == "Ridge":
        model = RidgeCV(alphas=[0.1, 1.0, 10.0])
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    print(f"Using {type(model).__name__} for {region}.")
    model.fit(X_scaled_df, target)

    forecast_date = eia_changes.index.max() + timedelta(days=7)

    forecast_features_dict = {}
    
    if total_col in eia_totals.columns:
        forecast_features_dict["StorageTotal"] = eia_totals[total_col].iloc[-1]
    else:
        forecast_features_dict["StorageTotal"] = 0

    for f_item in fundy_region_items:
        if f_item in fundy_weekly_resampled.columns and not fundy_weekly_resampled[f_item].empty:
            forecast_features_dict[f_item] = fundy_weekly_resampled[f_item].iloc[-1]
        else:
            forecast_features_dict[f_item] = 0 

    for w_col_base in base_weather_cols:
        actual_weather_col_name = f"Conus_{w_col_base}" if city_for_weather == "Conus" else f"{city_for_weather}_{w_col_base}"
        
        if actual_weather_col_name in weather_weekly_df.columns and not weather_weekly_df[actual_weather_col_name].empty:
            forecast_features_dict[w_col_base] = weather_weekly_df[actual_weather_col_name].iloc[-1]
        else:
            forecast_features_dict[w_col_base] = 0

        rolling_col_name = f"{w_col_base}_roll3"
        if rolling_col_name in df.columns and not df[rolling_col_name].empty:
            forecast_features_dict[rolling_col_name] = df[rolling_col_name].iloc[-1]
        else:
            forecast_features_dict[rolling_col_name] = 0

    # === Forecast Enhanced Weather Features (Conditional) ===
    if enhance_weather:
        for feature in ["CDD", "HDD"]:
            if feature in df.columns:
                forecast_features_dict[f"{feature}_lag1"] = df[feature].iloc[-1]
                
                delta_col_name = f"{feature}_delta5yr"
                last_5yr_roll_mean = df[feature].rolling(window=260, min_periods=1).mean().iloc[-1]
                forecast_features_dict[delta_col_name] = df[feature].iloc[-1] - last_5yr_roll_mean
            else:
                forecast_features_dict[f"{feature}_lag1"] = 0
                forecast_features_dict[f"{feature}_delta5yr"] = 0
    # ELSE: If enhance_weather is False, these enhanced features won't exist in `features.columns`
    # for the regions where it's off, so they won't be expected in `forecast_features_df`.
    # No explicit 'else' needed here to set to 0, as `forecast_features_df` alignment handles it.

    # === Forecast Criterion Data (Conditional) ===
    criterion_col_name = f"CriterionChange_{region.replace(' ', '')}"
    if use_criterion:
        if criterion_col_name in criterion_weekly.columns and not criterion_weekly[criterion_col_name].empty:
            forecast_features_dict[criterion_col_name] = criterion_weekly[criterion_col_name].iloc[-1]
        else:
            if criterion_col_name in features.columns: # Only add if trained with this column
                forecast_features_dict[criterion_col_name] = 0
    else:
        if criterion_col_name in features.columns: # Only add if trained with this column
            forecast_features_dict[criterion_col_name] = 0


    if "Target_roll3" in df.columns and not df["Target_roll3"].empty:
        forecast_features_dict["Target_roll3"] = df["Target_roll3"].iloc[-1]
    else:
        forecast_features_dict["Target_roll3"] = 0


    forecast_features_df = pd.DataFrame([forecast_features_dict])
    
    missing_cols = set(features.columns) - set(forecast_features_df.columns)
    for c in missing_cols:
        forecast_features_df[c] = 0 

    forecast_features_df = forecast_features_df[features.columns]

    forecast = np.nan
    try:
        forecast_scaled = scaler.transform(forecast_features_df)
        forecast = model.predict(forecast_scaled)[0]
    except Exception as e:
        print(f"⚠️ Forecast skipped for {region} due to error during prediction: {e}")
        print(f"Forecast features used:\n{forecast_features_df}")


    # --- Backtest ---
    valid_dates_for_backtest = df_train.index
    if len(valid_dates_for_backtest) > 4:
        backtest_dates = valid_dates_for_backtest[-4:].to_list()
    else:
        backtest_dates = valid_dates_for_backtest.to_list()

    preds, actuals = [], []
    backtest_details = []

    print(f"Attempting backtest for {region} on {len(backtest_dates)} dates.")
    for bt_date in backtest_dates:
        try:
            x_bt = scaler.transform(features.loc[[bt_date]])
            y_bt = target.loc[bt_date]
            y_pred = model.predict(x_bt)[0]
            preds.append(y_pred)
            actuals.append(y_bt)
            backtest_details.append(f"    {bt_date.date()} - Actual: {y_bt:+.2f}, Predicted: {y_pred:+.2f}")
        except KeyError:
            backtest_details.append(f"⚠️ Skipped backtest on {bt_date.date()} due to missing data for features or target.")
        except Exception as e:
            backtest_details.append(f"⚠️ Skipped backtest on {bt_date.date()} due to error: {e}")

    for detail in backtest_details:
        print(detail)

    if preds:
        mae = mean_absolute_error(actuals, preds)
        print(f"\n📈 Forecast for {forecast_date.date()}: {forecast:+.2f} Bcf")
        print(f"🧪 Backtest MAE (past {len(backtest_dates)} weeks): {mae:.2f} Bcf")
    else:
        mae = np.nan
        print(f"⚠️ No valid backtest for {region}")

    # --- LOG DEBUG INFO FOR LLM ANALYSIS ---
    debug_log = {
        "Region": region,
        "Latest Date": str(df_train.index.max().date()),
        "Training Sample Count": int(len(df_train)),
        "Backtest MAE": float(round(mae, 3)) if not np.isnan(mae) else "N/A",
        "Model Type": type(model).__name__,
        "ModelParameters": str(model), # Changed from "Model" to "ModelParameters"
        "Top Feature Coefficients": {},
        "Zero Coefficient Count": 0,
        "Total Feature Count": int(len(features.columns)),
        "Feature Names": list(features.columns),
        "UseCriterion": use_criterion, # Added this line
        "EnhanceWeather": enhance_weather # Added this line
    }

    if hasattr(model, 'coef_') and len(features.columns) == len(model.coef_):
        coefs = pd.Series(model.coef_, index=features.columns)
        nonzero_coefs = coefs[coefs != 0].sort_values(key=abs, ascending=False)
        debug_log["Top Feature Coefficients"] = {k: float(v) for k, v in nonzero_coefs.head(10).to_dict().items()}
        debug_log["Zero Coefficient Count"] = int((coefs == 0).sum())
        if hasattr(model, 'alpha_'):
            debug_log["Model Alpha"] = float(round(model.alpha_, 4))
        else:
            debug_log["Model Alpha"] = "N/A"
    else:
        debug_log["Top Feature Coefficients"] = "Model does not have coefficients or mismatch in length."
        debug_log["Zero Coefficient Count"] = "N/A"
        debug_log["Model Alpha"] = "N/A"
    
    # Identify top weather features and top fundamentals for the debug log
    # These are based on column names in the final 'df' after all joins
    debug_log["TopWeatherFeatures"] = [
        col for col in df.columns if any(s in col.lower() for s in ['temp', 'cdd', 'hdd', 'precip'])
    ]
    debug_log["TopFundamentals"] = [
        col for col in df.columns if any(s in col.lower() for s in ['fundy', 'total', 'storage', 'balance'])
        and 'storage' not in col.lower() # Exclude 'StorageTotal' here as it's often a key feature
    ]
    # Adjust TopFundamentals to specifically look for Fundy items more robustly
    debug_log["TopFundamentals"].extend([item for item in fundy_region_items if item not in debug_log["TopFundamentals"]])
    debug_log["TopFundamentals"] = sorted(list(set(debug_log["TopFundamentals"]))) # Remove duplicates and sort

    serializable_forecast_features = {k: float(v) if isinstance(v, (np.float32, np.float64)) else v for k, v in forecast_features_dict.items()}
    serializable_forecast_features = {k: int(v) if isinstance(v, np.int64) else v for k, v in serializable_forecast_features.items()}
    debug_log["Latest Forecast Features"] = serializable_forecast_features

    debug_log["Backtest Actuals"] = [float(round(a, 2)) for a in actuals]
    debug_log["Backtest Predicted"] = [float(round(p, 2)) for p in preds]
    debug_log["Backtest Dates"] = [str(d.date()) for d in backtest_dates]

    Path("GOScripts/GDebug").mkdir(parents=True, exist_ok=True)
    with open(f"GOScripts/GDebug/{region.replace(' ', '_')}_debug.json", "w") as f:
        json.dump(debug_log, f, indent=2)
    print(f"✅ Debug log saved for {region} to GOScripts/GDebug/{region.replace(' ', '_')}_debug.json")

    results.append({
        "Region": region,
        "Forecast Date": forecast_date.date(),
        "Forecast (Bcf)": round(forecast, 2) if not np.isnan(forecast) else "N/A",
        "Backtest MAE (Bcf)": round(mae, 2) if not np.isnan(mae) else "N/A",
        "Backtest Dates": [str(d.date()) for d in backtest_dates],
        "Backtest Actuals (Bcf)": [float(round(a, 2)) for a in actuals],
        "Backtest Predicted (Bcf)": [float(round(p, 2)) for p in preds]
    })

results_df = pd.DataFrame(results)
results_df["Backtest Dates"] = results_df["Backtest Dates"].apply(lambda x: str(x))
results_df["Backtest Actuals (Bcf)"] = results_df["Backtest Actuals (Bcf)"].apply(lambda x: str(x))
results_df["Backtest Predicted (Bcf)"] = results_df["Backtest Predicted (Bcf)"].apply(lambda x: str(x))

output_directory = "GOScripts/GOutput"
Path(output_directory).mkdir(parents=True, exist_ok=True)
output_filename = f"{output_directory}/EIAStoragePred.csv"
results_df.to_csv(output_filename, index=False)

print(f"\n✅ CSV saved: {output_filename}")
print("Columns in output:")
print(results_df.columns.tolist())