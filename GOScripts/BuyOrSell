import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict
import ast # Required for ast.literal_eval in load_clean_df

# --- Setup ---
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent
data_folder = project_root / "INFO" # This is the primary INFO folder
goscripts_folder = project_root / "GOScripts"

print(f"Data folder resolved to: {data_folder.resolve()}")
print(f"GOScripts folder resolved to: {goscripts_folder.resolve()}")

# Standard base temperature for CDD/HDD in natural gas markets
BASE_TEMP_CDD_HDD = 65

# --- City Name Standardization Map ---
CITY_STANDARDIZATION_MAP = {
    'JFK NY': 'John F. Kennedy NY',
    'Houston IAH TX': 'Houston TX',
    'Ok. City OK': 'Oklahoma City OK',
    'Chicago OHare IL': 'Chicago IL',
    'Washington DC': 'Washington National DC',
    'Ral-Durham NC': 'Raleigh/Durham NC',
    'Atlanta GA': 'Atlanta GA',
    'Boston MA': 'Boston MA',
    'Buffalo NY': 'Buffalo NY',
    'Denver CO': 'Denver CO',
    'Detroit MI': 'Detroit IL',
    'Los Angeles CA': 'Los Angeles CA',
    'Little Rock AR': 'Little Rock AR',
    'New Orleans LA': 'New Orleans LA',
    'Philadelphia PA': 'Philadelphia PA',
    'Pittsburgh PA': 'Pittsburgh PA',
    'Seattle WA': 'Seattle WA',
    'San Francisco CA': 'San Francisco CA',
    'Tampa FL': 'Tampa FL',
}

def standardize_city_name(city_name):
    """Applies a common standardization (strip) and then uses the hardcoded map."""
    city_name = str(city_name).strip()
    return CITY_STANDARDIZATION_MAP.get(city_name, city_name)

# --- Hardcoded Component to City Mapping ---
COMPONENT_TO_CITY_MAP_HARDCODED = {
    'AGT-CG (non-G)': 'John F. Kennedy NY',
    'ANR-SE-T': 'Houston TX',
    'ANR-SW': 'Oklahoma City OK',
    'APC-ACE': 'Chicago IL',
    'CG-Mainline': 'New Orleans LA',
    'CG-Onshore': 'New Orleans LA',
    'Carthage': 'Oklahoma City OK',
    'Chicago': 'Chicago IL',
    'Dracut': 'Boston MA',
    'Eastern Gas-South': 'Pittsburgh PA',
    'FGT-Z3': 'Tampa FL',
    'HSC-HPL Pool': 'Houston TX',
    'Henry': 'Henry Hub',
    'Iroquois (into)': 'Buffalo NY',
    'Iroquois-Z2': 'John F. Kennedy NY',
    'Leidy-Transco': 'Philadelphia PA',
    'Michcon': 'Detroit IL',
    'NBPL-Vector': 'Chicago IL',
    'NGPL-Midcont Pool': 'Oklahoma City OK',
    'NGPL-STX': 'Houston TX',
    'NGPL-TXOK East': 'Houston TX',
    'NNG-Demarc': 'Chicago IL',
    'NNG-Ventura': 'Chicago IL',
    'Panhandle': 'Oklahoma City OK',
    'Pine Prairie': 'Atlanta GA',
    'REX E-NGPL': 'Chicago IL',
    'REX-Z3 (receipt)': 'Philadelphia PA',
    'Sonat-Z0 South': 'Atlanta GA',
    'TCO': 'Pittsburgh PA',
    'TETCO-ELA': 'Houston TX',
    'TETCO-M2 (receipt)': 'Pittsburgh PA',
    'TETCO-M3': 'John F. Kennedy NY',
    'TETCO-STX': 'Houston TX',
    'TETCO-WLA': 'Houston TX',
    'TGP-500L': 'Houston TX',
    'TGP-800L': 'Houston TX',
    'TGP-Z0 South': 'Houston TX',
    'TGP-Z1 100L': 'Houston TX',
    'TGP-Z1 Sta-87': 'Houston TX',
    'TGP-Z4 Marcellus': 'Pittsburgh PA',
    'TGP-Z4 Sta-219': 'Pittsburgh PA',
    'TGP-Z4 Sta-313': 'Pittsburgh PA',
    'TGT-Mainline': 'Houston TX',
    'Transco Zn3': 'Atlanta GA',
    'Transco-165': 'Raleigh/Durham NC',
    'Transco-30': 'Houston TX',
    'Transco-45': 'Atlanta GA',
    'Transco-65': 'Atlanta GA',
    'Transco-85': 'Atlanta GA',
    'Transco-Z5 South': 'Washington National DC',
    'Transco-Z6 (NY)': 'John F. Kennedy NY',
    'Transco-Z6 (non-NY north)': 'Philadelphia PA',
    'Transco-Z6 (non-NY)': 'Philadelphia PA',
    'Transco-Z6 Sta-210': 'Philadelphia PA',
    'Trunkline-Z1A': 'Houston TX',
    'Union-Dawn': 'Buffalo NY',
    'Waha': 'Waha',
}

# --- Region and City Mappings for Storage and Weather ---
EAST_CITIES = [
    "Boston MA", "Buffalo NY", "John F. Kennedy NY",
    "Philadelphia PA", "Pittsburgh PA", "Washington National DC",
    "Raleigh/Durham NC"
]

MIDWEST_CITIES = [
    "Chicago IL", "Detroit IL"
]

SALT_SC_CITIES = [
    "Houston TX", "New Orleans LA"
]

NONSALT_SC_CITIES = [
    "Little Rock AR", "Oklahoma City OK", "Atlanta GA"
]

SOUTH_CENTRAL_CITIES = SALT_SC_CITIES + NONSALT_SC_CITIES

PACIFIC_CITIES = [
    "Los Angeles CA", "San Francisco CA", "Seattle WA"
]

MOUNTAIN_CITIES = [
    "Denver CO"
]

ALL_CITIES = sorted(list(set(
    EAST_CITIES + MIDWEST_CITIES + SOUTH_CENTRAL_CITIES + PACIFIC_CITIES + MOUNTAIN_CITIES
)))

WEATHER_REGION_CITY_MAP = {
    "East Region Storage (Bcf)": EAST_CITIES,
    "Midwest Region Storage (Bcf)": MIDWEST_CITIES,
    "Salt Region SC Storage (Bcf)": SALT_SC_CITIES,
    "Nonsalt Region SC Storage (Bcf)": NONSALT_SC_CITIES,
    "South Central Region Storage (Bcf)": SOUTH_CENTRAL_CITIES,
    "Pacific Region Storage (Bcf)": PACIFIC_CITIES,
    "Mountain Region Storage (Bcf)": MOUNTAIN_CITIES,
    "Lower 48 States Storage (Bcf)": ALL_CITIES
}

region_map = {
    "Lower 48 States Storage (Bcf)": "lower_48_states_storage",
    "East Region Storage (Bcf)": "east_region_storage",
    "Midwest Region Storage (Bcf)": "midwest_region_storage",
    "South Central Region Storage (Bcf)": "south_central_region_storage",
    "Salt Region SC Storage (Bcf)": "salt_region_sc_storage",
    "Nonsalt Region SC Storage (Bcf)": "nonsalt_region_sc_storage",
    "Mountain Region Storage (Bcf)": "mountain_region_storage",
    "Pacific Region Storage (Bcf)": "pacific_region_storage"
}

files = {
    "eia_totals": "EIAtotals.csv",
    "eia_changes": "EIAchanges.csv",
    "weather": "WEATHER.csv",
    "prices": "PRICES.csv",
    "fundy": "Fundy.csv",
    "forecast": "FundyForecast.csv",
    "criterion_storage_change": "CriterionStorageChange.csv",
    "criterion_nuclear_hist": "criterionnuclearhist.csv",
    "plant_group_mapping": "Plant_Group_Mapping.csv",
    "lng_hist": "CriterionLNGHist.csv",
    "natural_gas_data_positioning": "Natural_Gas_Data_Positioning.csv"
}

HENRY_HUB_NAME = 'Henry'

# --- Data Loading and Cleaning ---
def load_clean_df(file_key, fname, data_path):
    fpath = data_path / fname
    if not fpath.exists():
        print(f"❌ Error: Required file '{fname}' not found at {fpath}. Skipping this file.")
        return None
    try:
        if file_key == "lng_hist":
            df = pd.read_csv(fpath)
            df = df.rename(columns={
                'Date': 'date',
                'Flow_MMcfd': 'flow_mmcfd',
                'Value': 'flow_mmcfd'
            })
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            if 'flow_mmcfd' in df.columns:
                df['flow_mmcfd'] = pd.to_numeric(df['flow_mmcfd'], errors='coerce')
            return df
            
        elif file_key == "natural_gas_data_positioning":
            df = pd.read_csv(fpath)
            df = df.rename(columns={
                'Report Date': 'date',
                'Money Long': 'money_long', # These might not be present if "Managed Money" is net
                'Money Short': 'money_short', # These might not be present if "Managed Money" is net
                'Managed Money': 'managed_money_net' # Use this if it represents the net position directly
            })
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            if 'money_long' in df.columns:
                df['money_long'] = pd.to_numeric(df['money_long'], errors='coerce')
            if 'money_short' in df.columns:
                df['money_short'] = pd.to_numeric(df['money_short'], errors='coerce')
            if 'managed_money_net' in df.columns:
                df['managed_money_net'] = pd.to_numeric(df['managed_money_net'], errors='coerce')
            return df

        df = pd.read_csv(fpath)
    except Exception as e:
        print(f"❌ Error reading CSV file '{fname}' at {fpath}: {e}. Skipping this file.")
        return None

    if file_key in ["eia_totals", "eia_changes"]:
        df.columns = df.columns.str.lower().str.replace(" (bcf)", "", regex=False).str.replace(" ", "_").str.replace(":", "").str.strip()
        if "period" in df.columns:
            df["period"] = pd.to_datetime(df["period"], errors="coerce")
    elif file_key == "weather":
        df.columns = df.columns.str.strip()
        if 'Date' in df.columns:
            df = df.rename(columns={'Date': 'date'})
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if 'City Title' in df.columns:
            df = df.rename(columns={'City Title': 'city_title'})
            df['city_title'] = df['city_title'].astype(str).apply(standardize_city_name)
        if 'Avg Temp' in df.columns:
            df = df.rename(columns={'Avg Temp': 'avg_temp'})
        if 'avg_temp' in df.columns:
            df['avg_temp'] = pd.to_numeric(df['avg_temp'], errors='coerce')
    elif file_key == "prices":
        df.columns = df.columns.str.strip()
        if 'Date' in df.columns:
            df = df.rename(columns={'Date': 'date'})
            df['date'] = pd.to_datetime(df['date'], errors='coerce')

        henry_col_found = False
        potential_henry_cols = [col for col in df.columns if 'Henry Hub' in col or 'Henry' == col]
        if HENRY_HUB_NAME in potential_henry_cols:
            df = df.rename(columns={HENRY_HUB_NAME: 'henry'})
            henry_col_found = True
        elif potential_henry_cols:
            df = df.rename(columns={potential_henry_cols[0]: 'henry'})
            henry_col_found = True

        if henry_col_found:
            df['henry'] = pd.to_numeric(df['henry'], errors='coerce')
    elif file_key in ["fundy", "forecast"]:
        df.columns = df.columns.str.strip()
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors="coerce")
    elif file_key == "criterion_storage_change":
        df.columns = df.columns.str.strip()
        if 'eff_gas_day' in df.columns:
            df['eff_gas_day'] = pd.to_datetime(df['eff_gas_day'], errors="coerce")
        if 'daily_storage_change' in df.columns:
            df['daily_storage_change'] = pd.to_numeric(df['daily_storage_change'], errors="coerce")
            df_cleaned = pd.DataFrame()
            if not df.empty:
                for (storage_name, year), group in df.groupby([df['storage_name'], df['eff_gas_day'].dt.year]):
                    group = group.copy()
                    if not group['daily_storage_change'].dropna().empty:
                        min_val_group_year = group['daily_storage_change'].min()
                        group['daily_storage_change'] = group['daily_storage_change'] - min_val_group_year
                    df_cleaned = pd.concat([df_cleaned, group])
                df = df_cleaned.sort_values(by=['storage_name', 'eff_gas_day']).reset_index(drop=True)
            current_year = datetime.now().year
            df = df[df['eff_gas_day'].dt.year == 2025].copy()
    elif file_key in ["criterion_nuclear_hist", "nuclear_forecast"]:
        df.columns = df.columns.str.strip()
        if 'Date' in df.columns:
            df = df.rename(columns={'Date': 'date'})
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if 'Value' in df.columns:
            df = df.rename(columns={'Value': 'mw_generated'})
        elif 'value' in df.columns:
            df = df.rename(columns={'value': 'mw_generated'})
        else:
            return None
        if 'mw_generated' in df.columns:
            df['mw_generated'] = pd.to_numeric(df['mw_generated'], errors='coerce')
        if 'Type' in df.columns:
            df['type'] = df['Type'].str.strip()
        if 'Item' in df.columns:
            df['item'] = df['Item'].str.strip()
        elif 'item' not in df.columns:
            df['item'] = 'Unknown Nuclear Plant'
    elif file_key == "plant_group_mapping":
        df.columns = df.columns.str.strip()
        if 'Units' in df.columns and 'Group' in df.columns:
            try:
                df["Units"] = df["Units"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
                df["Units"] = df["Units"].apply(lambda x: x if isinstance(x, list) else [x])
            except (ValueError, SyntaxError) as e:
                print(f"⚠️ Warning: Could not parse 'Units' column in '{fname}'. Ensure it contains valid list-like strings. Error: {e}")
                return None
        else:
            return None
    return df

# --- Simplified Storage Position Calculation (with std_dev for Z-score) ---
def get_storage_position(display_name, internal_column_name, raw_data, eia_report_date):
    """Calculates storage position relative to 5-year average and standard deviation for a given region."""
    merged = pd.DataFrame()
    if "eia_totals" in raw_data and "eia_changes" in raw_data:
        merged = pd.merge(raw_data["eia_totals"], raw_data["eia_changes"], on="period", how="outer")
        merged = merged.sort_values("period")
    else:
        return None # Return None if essential data is missing

    if internal_column_name not in merged.columns:
        return None

    df_for_latest = merged.dropna(subset=[internal_column_name])
    if df_for_latest.empty:
        return None

    latest_valid_row_series = df_for_latest.iloc[-1]
    current_storage_value = latest_valid_row_series.get(internal_column_name)

    storage_position_bcf_val = None
    std_dev_bcf_val = None

    if pd.notna(current_storage_value):
        current_storage = round(current_storage_value, 1)
        current_week = latest_valid_row_series["period"].isocalendar().week
        historical_data_for_week = merged[
            (merged["period"].dt.isocalendar().week == current_week) &
            (merged["period"].dt.year < datetime.now().year)
        ]

        min_years_for_avg = 5
        num_historical_years = historical_data_for_week["period"].dt.year.nunique()

        if num_historical_years >= min_years_for_avg:
            mean_bcf_for_week = historical_data_for_week[internal_column_name].mean()
            std_bcf_for_week = historical_data_for_week[internal_column_name].std()

            if pd.notna(mean_bcf_for_week):
                five_year_avg = round(mean_bcf_for_week, 1)
                storage_position_bcf_val = current_storage - five_year_avg
                if pd.notna(std_bcf_for_week):
                    std_dev_bcf_val = std_bcf_for_week

    return {
        "storage_position_bcf": storage_position_bcf_val,
        "std_dev_bcf": std_dev_bcf_val,
        "display_name": display_name
    }

# --- Nuclear Outage Detection ---
def get_nuclear_outage_impact(nuclear_hist_df, plant_group_df, check_date, days_threshold=3, drop_percentage_threshold=5):
    """
    Detects if there's a significant nuclear generation drop for any group
    around the check_date.

    Returns the maximum average drop percentage detected in any group
    over the last 'days_threshold' days, or 0 if no qualifying drop.
    """
    if nuclear_hist_df is None or nuclear_hist_df.empty or plant_group_df is None or plant_group_df.empty:
        return 0

    nuclear_df_grouped = nuclear_hist_df.rename(columns={"item": "Unit", "mw_generated": "Output"})

    expanded_group_map = []
    if 'Units' in plant_group_df.columns and 'Group' in plant_group_df.columns:
        for _, row in plant_group_df.iterrows():
            group = row["Group"]
            units = row["Units"]
            if isinstance(units, str):
                try: units = ast.literal_eval(units)
                except (ValueError, SyntaxError): units = [units]
            elif not isinstance(units, list): units = [units]
            for unit in units:
                expanded_group_map.append({"Group": group, "Unit": unit})
    group_unit_df = pd.DataFrame(expanded_group_map)

    nuclear_df_grouped = nuclear_df_grouped.merge(group_unit_df, on="Unit", how="left")
    nuclear_df_grouped = nuclear_df_grouped.dropna(subset=["Group"])
    nuclear_df_grouped = nuclear_df_grouped[
        ~nuclear_df_grouped['Unit'].astype(str).str.contains('total|all', case=False, na=False)
    ].copy()

    if nuclear_df_grouped.empty:
        return 0

    group_daily_output = nuclear_df_grouped.groupby(["date", "Group"])["Output"].sum().reset_index()
    group_daily_output = group_daily_output.sort_values(["Group", "date"])

    group_daily_output["Baseline"] = (
        group_daily_output.groupby("Group")["Output"]
        .transform(lambda x: x.shift(1).rolling(window=7, min_periods=1).mean())
    )

    group_daily_output["Drop %"] = np.where(
        (group_daily_output["Baseline"] != 0) & (~group_daily_output["Baseline"].isna()),
        ((group_daily_output["Baseline"] - group_daily_output["Output"]) / group_daily_output["Baseline"]) * 100,
        np.nan
    )
    group_daily_output["Drop %"] = np.where(
        (group_daily_output["Output"] == 0) & (group_daily_output["Baseline"] > 0),
        100,
        group_daily_output["Drop %"]
    )
    group_daily_output["Drop %"] = group_daily_output["Drop %"].fillna(0)

    max_overall_drop = 0
    for group_name in group_daily_output['Group'].unique():
        group_data = group_daily_output[group_daily_output['Group'] == group_name].set_index('date').sort_index()
        recent_group_data = group_data.loc[check_date - timedelta(days=days_threshold-1):check_date]

        if not recent_group_data.empty and len(recent_group_data) >= days_threshold:
            if (recent_group_data['Drop %'] > drop_percentage_threshold).all():
                max_overall_drop = max(max_overall_drop, recent_group_data['Drop %'].mean())
    return max_overall_drop

# NEW: LNG Flow Impact Detection
def get_lng_flow_impact(lng_hist_df, check_date, days_threshold=3, drop_percentage_threshold=5):
    """
    Detects if there's a significant LNG flow drop (> X% over Y days).
    Assumes lng_hist_df has 'date' and 'flow_mmcfd' columns.

    Returns the maximum average drop percentage detected, or 0 if no qualifying drop.
    """
    if lng_hist_df is None or lng_hist_df.empty or \
       'date' not in lng_hist_df.columns or 'flow_mmcfd' not in lng_hist_df.columns:
        print("Warning: LNG historical flow data not available or missing required columns for drop detection.")
        return 0

    df_lng = lng_hist_df.dropna(subset=['date', 'flow_mmcfd']).copy()
    df_lng = df_lng.sort_values('date').set_index('date')

    # Calculate a 7-day rolling baseline (prior days only)
    df_lng["Baseline"] = df_lng["flow_mmcfd"].shift(1).rolling(window=7, min_periods=1).mean()

    # Calculate percentage drop from baseline
    df_lng["Drop %"] = np.where(
        (df_lng["Baseline"] != 0) & (~df_lng["Baseline"].isna()),
        ((df_lng["Baseline"] - df_lng["flow_mmcfd"]) / df_lng["Baseline"]) * 100,
        np.nan
    )
    # Handle cases where current flow is 0 and baseline is >0 as 100% drop
    df_lng["Drop %"] = np.where(
        (df_lng["flow_mmcfd"] == 0) & (df_lng["Baseline"] > 0),
        100,
        df_lng["Drop %"]
    )
    df_lng["Drop %"] = df_lng["Drop %"].fillna(0) # Fill other NaNs with 0

    max_overall_lng_drop = 0
    # Check for drops over the specified duration ending at check_date
    recent_lng_data = df_lng.loc[check_date - timedelta(days=days_threshold-1):check_date]

    if not recent_lng_data.empty and len(recent_lng_data) >= days_threshold:
        if (recent_lng_data['Drop %'] > drop_percentage_threshold).all():
            max_overall_lng_drop = recent_lng_data['Drop %'].mean() # Return average drop during qualifying period

    return max_overall_lng_drop

# NEW: Historical Outage Risk Prediction Functions
def get_historical_outage_risk(hist_df, id_col, value_col, check_date, lookahead_days=30, min_drop_pct=5, min_drop_duration=3):
    """
    Analyzes historical data for a specific period (e.g., next 30 days) to assess
    the average percentage drop for qualifying events.
    Applies to both nuclear and LNG.

    Args:
        hist_df (pd.DataFrame): The historical data (nuclear_hist_df or lng_hist_df).
        id_col (str): Column representing the individual unit/group ID ('Group' for nuclear, 'Item' or similar for LNG if applicable).
        value_col (str): Column with the generation/flow value ('Output' for nuclear, 'flow_mmcfd' for LNG).
        check_date (datetime.date): The current analysis date (used for finding corresponding period in historical years).
        lookahead_days (int): Number of days to look ahead into the historical period.
        min_drop_pct (int/float): Minimum percentage drop to consider an event.
        min_drop_duration (int): Minimum duration in days for a qualifying event.

    Returns:
        float: Average percentage drop of qualifying events during the historical lookahead periods.
               Returns 0 if no historical data or no qualifying events found.
    """
    if hist_df is None or hist_df.empty or \
       'date' not in hist_df.columns or value_col not in hist_df.columns:
        return 0

    df_temp = hist_df.dropna(subset=['date', value_col]).copy()
    df_temp['date'] = pd.to_datetime(df_temp['date'])
    df_temp[value_col] = pd.to_numeric(df_temp[value_col], errors='coerce')
    df_temp.dropna(subset=[value_col], inplace=True)

    if df_temp.empty:
        return 0

    historical_drops = []
    # Use month and day from check_date (which is today_date + 1 for lookahead)
    current_month_day_of_lookahead_start = (check_date.month, check_date.day)

    historical_years = sorted(df_temp['date'].dt.year.unique())
    current_year_in_data = check_date.year

    for year in historical_years:
        if year >= current_year_in_data:
            continue

        try:
            # Construct the start date for the lookahead period in the historical year
            hist_lookahead_start = datetime(year, current_month_day_of_lookahead_start[0], current_month_day_of_lookahead_start[1]).date()
        except ValueError: # Handle Feb 29 in a non-leap year gracefully
            if current_month_day_of_lookahead_start == (2, 29):
                hist_lookahead_start = datetime(year, 2, 28).date()
            else:
                continue

        hist_lookahead_end = hist_lookahead_start + timedelta(days=lookahead_days -1)

        window_data = df_temp[(df_temp['date'].dt.date >= hist_lookahead_start) &
                              (df_temp['date'].dt.date <= hist_lookahead_end)].copy()

        if window_data.empty:
            continue

        # Group and calculate baseline/drop percentage similar to current outage detection
        if id_col and id_col in window_data.columns:
            grouped_data = window_data.groupby(['date', id_col])[value_col].sum().reset_index()
            grouped_data = grouped_data.sort_values([id_col, 'date'])
            grouped_data["Baseline"] = grouped_data.groupby(id_col)[value_col].transform(lambda x: x.shift(1).rolling(window=7, min_periods=1).mean())
        else: # For overall LNG flow (no grouping by 'Item' unless needed for breakdown)
            grouped_data = window_data.groupby('date')[value_col].sum().reset_index()
            grouped_data = grouped_data.sort_values('date')
            grouped_data["Baseline"] = grouped_data[value_col].shift(1).rolling(window=7, min_periods=1).mean()

        grouped_data["Drop %"] = np.where(
            (grouped_data["Baseline"] != 0) & (~grouped_data["Baseline"].isna()),
            ((grouped_data["Baseline"] - grouped_data[value_col]) / grouped_data["Baseline"]) * 100,
            np.nan
        )
        grouped_data["Drop %"] = np.where(
            (grouped_data[value_col] == 0) & (grouped_data["Baseline"] > 0),
            100,
            grouped_data["Drop %"]
        )
        grouped_data["Drop %"] = grouped_data["Drop %"].fillna(0)

        grouped_data['Outage_Flag'] = grouped_data["Drop %"] > min_drop_pct
        grouped_data['block'] = (grouped_data['Outage_Flag'] != grouped_data['Outage_Flag'].shift()).cumsum()
        
        qualifying_outage_blocks = grouped_data[grouped_data['Outage_Flag']].groupby('block').filter(
            lambda x: len(x) >= min_drop_duration and x['Outage_Flag'].all()
        )

        if not qualifying_outage_blocks.empty:
            historical_drops.extend(qualifying_outage_blocks['Drop %'].tolist())

    if historical_drops:
        return np.mean(historical_drops)
    return 0


# --- Core Component Analysis Logic ---
def analyze_components(today_date, raw_data, storage_positions):
    """
    Analyzes various market components based on the specified criteria, scores them,
    and returns the top 5 sorted components.
    """
    components_analysis = {}
    top_5_summary_list = [] # ENHANCEMENT 4: For final polish summary

    prices_df = raw_data.get("prices")
    weather_df = raw_data.get("weather")
    eia_totals_df = raw_data.get("eia_totals")
    lng_hist_df = raw_data.get("lng_hist")
    nuclear_hist_df = raw_data.get("criterion_nuclear_hist")
    plant_group_mapping_df = raw_data.get("plant_group_mapping")
    natural_gas_data_positioning_df = raw_data.get("natural_gas_data_positioning")
    forecast_df = raw_data.get("forecast")

    # --- DIAGNOSTIC CHECKS ---
    print("\n--- Diagnostic: DataFrames Status ---")
    critical_dfs = {
        "prices_df": prices_df,
        "weather_df": weather_df,
        "eia_totals_df": eia_totals_df,
        "lng_hist_df": lng_hist_df,
        "nuclear_hist_df": nuclear_hist_df,
        "plant_group_mapping_df": plant_group_mapping_df,
        "natural_gas_data_positioning_df": natural_gas_data_positioning_df,
        "forecast_df": forecast_df
    }
    for df_name, df in critical_dfs.items():
        if df is None:
            print(f"  {df_name}: NOT LOADED (Check file path/name/errors during load)")
        elif df.empty:
            print(f"  {df_name}: LOADED BUT EMPTY")
        else:
            print(f"  {df_name}: Loaded, {len(df)} rows. Columns: {df.columns.tolist()}")

    if prices_df is None or weather_df is None or eia_totals_df is None or \
       prices_df.empty or weather_df.empty or eia_totals_df.empty:
        print("\nFATAL: One or more critical dataframes (prices, weather, EIA totals) are missing or empty. Cannot proceed with analysis.")
        return []

    # Ensure date columns are datetime objects for critical DFs
    prices_df['date'] = pd.to_datetime(prices_df['date'], errors='coerce')
    weather_df['date'] = pd.to_datetime(weather_df['date'], errors='coerce')
    eia_totals_df['period'] = pd.to_datetime(eia_totals_df['period'], errors='coerce')

    # Drop rows where date parsing failed in critical DFs
    prices_df.dropna(subset=['date'], inplace=True)
    weather_df.dropna(subset=['date'], inplace=True)
    eia_totals_df.dropna(subset=['period'], inplace=True)


    # Get data for the last 7 days ending today
    seven_days_ago = today_date - timedelta(days=6)

    # Filter prices for the last 7 days
    recent_prices = prices_df[(prices_df['date'].dt.date >= seven_days_ago) &
                              (prices_df['date'].dt.date <= today_date)].copy()
    if recent_prices.empty:
        print("Warning: No recent price data for the last 7 days. Price momentum might be N/A for all components.")

    # Filter weather for the last 7 days
    recent_weather = weather_df[(weather_df['date'].dt.date >= seven_days_ago) &
                                (weather_df['date'].dt.date <= today_date)].copy()
    if recent_weather.empty:
        print("Warning: No recent weather data for the last 7 days. Temperature/HDD/CDD will be N/A.")

    # Calculate overall Henry Hub price momentum
    henry_col_name_lower = HENRY_HUB_NAME.lower()
    if henry_col_name_lower not in prices_df.columns and HENRY_HUB_NAME in prices_df.columns:
        prices_df.rename(columns={HENRY_HUB_NAME: henry_col_name_lower}, inplace=True)

    today_henry_price = np.nan
    last_5_day_avg_henry_price = np.nan
    if not recent_prices.empty and henry_col_name_lower in recent_prices.columns:
        today_henry_price_series = recent_prices[recent_prices['date'].dt.date == today_date][henry_col_name_lower]
        today_henry_price = today_henry_price_series.iloc[0] if not today_henry_price_series.empty else np.nan

        last_5_day_prices_for_avg = recent_prices[(recent_prices['date'].dt.date >= today_date - timedelta(days=4)) &
                                                  (recent_prices['date'].dt.date <= today_date)][henry_col_name_lower]
        last_5_day_avg_henry_price = last_5_day_prices_for_avg.mean()


    # Calculate LNG feed gas drop impact from CriterionLNGHist.csv
    lng_flow_drop_impact = get_lng_flow_impact(lng_hist_df, today_date)
    if lng_flow_drop_impact > 0:
        print(f"Detected LNG flow drop of {lng_flow_drop_impact:.1f}% over 3+ days ending {today_date}.")


    # Calculate nearby nuclear generation drop
    nuclear_generation_drop_impact = get_nuclear_outage_impact(nuclear_hist_df, plant_group_mapping_df, today_date)

    # --- ENHANCEMENT 3: Improve Managed Money Score ---
    net_managed_money_change = 0
    if natural_gas_data_positioning_df is not None and not natural_gas_data_positioning_df.empty and \
       'date' in natural_gas_data_positioning_df.columns and \
       'managed_money_net' in natural_gas_data_positioning_df.columns: # Rely on 'managed_money_net' as the source

        df_mm = natural_gas_data_positioning_df.dropna(subset=['date', 'managed_money_net']).copy()
        df_mm['date'] = pd.to_datetime(df_mm['date']) # Ensure date is datetime for sorting
        df_mm = df_mm.sort_values('date')

        if len(df_mm) >= 5: # Need at least 5 data points for iloc[-5]
            latest_mm = df_mm["managed_money_net"].iloc[-1]
            prior_mm = df_mm["managed_money_net"].iloc[-5] # As per user's suggestion (5 rows ago)
            net_managed_money_change = latest_mm - prior_mm
        elif len(df_mm) >= 2: # Fallback to latest vs previous if less than 5 points
            latest_mm = df_mm["managed_money_net"].iloc[-1]
            prior_mm = df_mm["managed_money_net"].iloc[-2]
            net_managed_money_change = latest_mm - prior_mm
            print(f"Warning: Less than 5 Managed Money data points ({len(df_mm)}) for comparison. Using latest vs previous point.")
        else:
            print(f"Warning: Insufficient Managed Money data points ({len(df_mm)}) for change calculation.")
    else:
        print(f"Warning: Managed Money data not available or missing required 'date' and 'managed_money_net' columns.")


    # Calculate historical risk of upcoming outages (next 30 days)
    lookahead_start_date = today_date + timedelta(days=1)
    lookahead_days = 30 # Default lookahead period

    historical_nuclear_risk_pct = get_historical_outage_risk(
        nuclear_hist_df, id_col='Group', value_col='Output',
        check_date=lookahead_start_date, lookahead_days=lookahead_days
    )
    if historical_nuclear_risk_pct > 0:
        print(f"Historical Nuclear Outage Risk for next {lookahead_days} days: Avg drop {historical_nuclear_risk_pct:.1f}%")

    historical_lng_risk_pct = get_historical_outage_risk(
        lng_hist_df, id_col='Item', value_col='flow_mmcfd', # Assuming 'Item' column for LNG facility ID
        check_date=lookahead_start_date, lookahead_days=lookahead_days
    )
    if historical_lng_risk_pct > 0:
        print(f"Historical LNG Outage Risk for next {lookahead_days} days: Avg drop {historical_lng_risk_pct:.1f}%")


    # --- ENHANCEMENT 5: Add Forecasted Signal Using FundyForecast.csv ---
    avg_future_balance_signal = "Neutral balance forecast"
    forecast_signal_score = 0
    if forecast_df is not None and not forecast_df.empty and \
       'Date' in forecast_df.columns and 'item' in forecast_df.columns and 'value' in forecast_df.columns:
        
        forecast_df['Date'] = pd.to_datetime(forecast_df['Date'], errors='coerce')
        forecast_df.dropna(subset=['Date', 'item', 'value'], inplace=True)
        
        # Filter for "Balance" items and next 7 days
        next_7_days_start = today_date + timedelta(days=1)
        next_7_days_end = today_date + timedelta(days=7)

        future_balance_data = forecast_df[
            (forecast_df['item'].str.contains("Balance", case=False, na=False)) &
            (forecast_df['Date'].dt.date >= next_7_days_start) &
            (forecast_df['Date'].dt.date <= next_7_days_end)
        ].copy()

        if not future_balance_data.empty:
            avg_future_balance = future_balance_data["value"].mean()

            if pd.notna(avg_future_balance):
                if avg_future_balance < -50000:
                    avg_future_balance_signal = "Market expected to tighten (Bullish)"
                    forecast_signal_score = 1
                elif avg_future_balance > 50000:
                    avg_future_balance_signal = "Market expected to loosen (Bearish)"
                    forecast_signal_score = -1
                else:
                    avg_future_balance_signal = "Neutral balance forecast"
            else:
                avg_future_balance_signal = "Neutral balance forecast (Avg is NaN)"
        else:
            print("Warning: No future balance data found in FundyForecast.csv for next 7 days.")
            avg_future_balance_signal = "Neutral balance forecast (No data)"
    else:
        print("Warning: FundyForecast.csv not loaded or missing required columns for forecasted signal.")
        avg_future_balance_signal = "Neutral balance forecast (File missing)"


    # Iterate through each component (columns in prices_df, excluding 'date', 'henry', and other unwanted cols)
    component_price_columns = prices_df.columns.drop(['date', henry_col_name_lower, 'Unnamed: 58', 'Date.1'], errors='ignore')

    if component_price_columns.empty:
        print("\nWarning: No component price columns found in PRICES.csv after filtering. Is Henry Hub named correctly or are other price columns present?")

    for component_name in component_price_columns:
        component_scores = {}
        summary_parts = []
        overall_score = 0

        # Match to region and weather city
        weather_city = COMPONENT_TO_CITY_MAP_HARDCODED.get(component_name)
        if not weather_city:
            continue

        # Pull recent data for component
        # Ensure enough data for 10-day basis deviation (needs 11 points: today + 10 prior)
        price_series_for_component = prices_df[[component_name, 'date']].dropna().set_index('date').sort_index()

        # 1. ΔStorage (vs 5-yr avg) - Scaled by Z-score
        delta_storage_score = 0
        storage_info_for_component = None
        for region_display_name, cities_in_region in WEATHER_REGION_CITY_MAP.items():
            if weather_city in cities_in_region:
                storage_info_for_component = storage_positions.get(region_display_name)
                break

        if storage_info_for_component and pd.notna(storage_info_for_component['storage_position_bcf']):
            delta_storage_bcf = storage_info_for_component['storage_position_bcf']
            std_dev_bcf = storage_info_for_component['std_dev_bcf']

            if pd.notna(std_dev_bcf) and std_dev_bcf > 0: # Check for non-zero std_dev
                z_score = delta_storage_bcf / std_dev_bcf
                if z_score < -1.5:
                    delta_storage_score = 2
                    summary_parts.append(f"Storage Deficit: {delta_storage_bcf:,.1f} Bcf (Z={z_score:.2f}) (Bullish)")
                elif z_score < -0.5:
                    delta_storage_score = 1
                    summary_parts.append(f"Storage Deficit: {delta_storage_bcf:,.1f} Bcf (Z={z_score:.2f}) (Bullish)")
                elif z_score > 1.5:
                    delta_storage_score = -2
                    summary_parts.append(f"Storage Surplus: +{delta_storage_bcf:,.1f} Bcf (Z={z_score:.2f}) (Bearish)")
                elif z_score > 0.5:
                    delta_storage_score = -1
                    summary_parts.append(f"Storage Surplus: +{delta_storage_bcf:,.1f} Bcf (Z={z_score:.2f}) (Bearish)")
                else:
                    summary_parts.append("Storage In Line")
            else: # Fallback if std_dev is NaN or zero (e.g., all historical values are same)
                if delta_storage_bcf < -100: delta_storage_score = 4
                elif delta_storage_bcf < -20: delta_storage_score = 2
                elif delta_storage_bcf > 100: delta_storage_score = -4
                elif delta_storage_bcf > 20: delta_storage_score = -2
                summary_parts.append(f"Storage Position: {delta_storage_bcf:,.1f} Bcf (No Z-score or zero std_dev)")
        else:
            summary_parts.append("Storage Data N/A")
        component_scores["ΔStorage"] = delta_storage_score

        # 2. ΔTemp / ΔHDD/CDD (using 7-day totals)
        temp_cdd_score = 0
        city_recent_weather_data = recent_weather[recent_weather['city_title'] == weather_city].copy()
        if not city_recent_weather_data.empty:
            avg_temp = city_recent_weather_data['avg_temp'].mean()
            total_cdd = (city_recent_weather_data['avg_temp'] - BASE_TEMP_CDD_HDD).apply(lambda x: max(0, x)).sum()
            total_hdd = (BASE_TEMP_CDD_HDD - city_recent_weather_data['avg_temp']).apply(lambda x: max(0, x)).sum()

            if total_cdd > 30:
                temp_cdd_score = 3
                summary_parts.append(f"High CDDs ({total_cdd:.1f}) (Bullish Weather)")
            elif total_hdd > 30:
                temp_cdd_score = 3
                summary_parts.append(f"High HDDs ({total_hdd:.1f}) (Bullish Weather)")
            elif total_cdd > 10:
                temp_cdd_score = 1
                summary_parts.append(f"Moderate CDDs ({total_cdd:.1f}) (Bullish Weather)")
            elif total_hdd > 10:
                temp_cdd_score = 1
                summary_parts.append(f"Moderate HDDs ({total_hdd:.1f}) (Bullish Weather)")
            else:
                summary_parts.append(f"Neutral Temps ({avg_temp:.1f}°F)")
        else:
            summary_parts.append("Weather Data N/A")
        component_scores["ΔTemp/HDD/CDD"] = temp_cdd_score

        # 3. Price momentum (today vs last 5-day avg for *component's own price*)
        price_momentum_score = 0
        # Ensure enough data points for the 5-day average
        price_data_for_momentum = price_series_for_component[component_name].tail(5).dropna()
        if len(price_data_for_momentum) >= 5:
            today_comp_price = price_data_for_momentum.iloc[-1]
            last_5_day_avg_comp_price = price_data_for_momentum.mean()
            price_momentum_comp = today_comp_price - last_5_day_avg_comp_price

            if price_momentum_comp > 0.05:
                price_momentum_score = 2
                summary_parts.append(f"Strong Price Momentum ({price_momentum_comp:+.2f}) (Bullish)")
            elif price_momentum_comp < -0.05:
                price_momentum_score = -2
                summary_parts.append(f"Negative Price Momentum ({price_momentum_comp:+.2f}) (Bearish)")
            else:
                summary_parts.append("Stable Price Momentum")
        else:
            summary_parts.append("Price Momentum N/A (Insufficient data)")
        component_scores["Price Momentum"] = price_momentum_score

        # --- ENHANCEMENT 4: Add Basis Deviation Check ---
        basis_deviation_score = 0
        basis_delta = np.nan
        # Ensure at least 11 points: 1 (today) + 10 (for average)
        price_series_for_basis = price_series_for_component[component_name].tail(11).dropna()
        
        if len(price_series_for_basis) >= 11:
            basis_today_val = price_series_for_basis.iloc[-1]
            basis_avg_val = price_series_for_basis[:-1].mean() # Mean of the preceding 10 days
            basis_delta = basis_today_val - basis_avg_val

            if basis_delta > 0.2:
                basis_deviation_score = 1
                summary_parts.append(f"Positive Basis Deviation ({basis_delta:+.2f}) (Bullish)")
            elif basis_delta < -0.2:
                basis_deviation_score = -1
                summary_parts.append(f"Negative Basis Deviation ({basis_delta:+.2f}) (Bearish)")
            else:
                summary_parts.append("Stable Basis Deviation")
        else:
            summary_parts.append("Basis Deviation N/A (Insufficient data)")
        component_scores["Basis Deviation"] = basis_deviation_score


        # ΔLNG feed gas nearby (NOW uses percentage drop logic)
        lng_score = 0
        if lng_flow_drop_impact > 10:
            lng_score = -3
            summary_parts.append(f"Significant Current LNG Flow Drop ({lng_flow_drop_impact:.1f}%) (Bearish)")
        elif lng_flow_drop_impact > 0:
            lng_score = -1
            summary_parts.append(f"Minor Current LNG Flow Drop ({lng_flow_drop_impact:.1f}%) (Slightly Bearish)")
        else:
            summary_parts.append("Stable Current LNG Flow")
        component_scores["ΔLNG Feed Gas"] = lng_score

        # Nearby nuclear generation drop (>5% over 3+ days)
        nuclear_score = 0
        if nuclear_generation_drop_impact > 10:
            nuclear_score = 3
            summary_parts.append(f"Significant Current Nuclear Gen. Drop ({nuclear_generation_drop_impact:.1f}%) (Bullish)")
        elif nuclear_generation_drop_impact > 0:
            nuclear_score = 1
            summary_parts.append(f"Minor Current Nuclear Gen. Drop ({nuclear_generation_drop_impact:.1f}%) (Bullish)")
        else:
            summary_parts.append("Stable Current Nuclear Generation")
        component_scores["Nuclear Drop"] = nuclear_score

        # 7. Net change in Managed Money positioning (Improved calculation)
        managed_money_score = 0
        if net_managed_money_change > 10000:
            managed_money_score = 1
            summary_parts.append(f"Managed Money Net Long Increase ({net_managed_money_change:+.0f} contracts) (Bullish)")
        elif net_managed_money_change < -10000:
            managed_money_score = -1
            summary_parts.append(f"Managed Money Net Short Increase ({net_managed_money_change:+.0f} contracts) (Bearish)")
        else:
            summary_parts.append("Managed Money Positioning Stable")
        component_scores["Managed Money"] = managed_money_score

        # 8. Upcoming Outage Risk (Historical)
        upcoming_outage_risk_score = 0
        risk_desc_parts = []
        if historical_nuclear_risk_pct > 10:
            upcoming_outage_risk_score += 1
            risk_desc_parts.append(f"Nuclear ({historical_nuclear_risk_pct:.1f}%)")
        if historical_lng_risk_pct > 10:
            upcoming_outage_risk_score += 1
            risk_desc_parts.append(f"LNG ({historical_lng_risk_pct:.1f}%)")

        if upcoming_outage_risk_score > 0:
            summary_parts.append(f"High Historical Outage Risk Soon ({', '.join(risk_desc_parts)}) (Bullish)")
        else:
            summary_parts.append("Low Historical Outage Risk Soon")
        component_scores["Upcoming Outage Risk"] = upcoming_outage_risk_score

        # 9. Forecasted Signal Using FundyForecast.csv
        summary_parts.append(f"Forecast: {avg_future_balance_signal}")
        component_scores["Forecasted Balance"] = forecast_signal_score


        # Calculate total score
        overall_score = sum(component_scores.values())

        # --- ENHANCEMENT 1: Add Bias Tag ---
        bias_tag = "UNKNOWN"
        if overall_score >= 5:
            bias_tag = "STRONG BUY"
        elif overall_score >= 3:
            bias_tag = "BUY"
        elif overall_score >= 1:
            bias_tag = "NEUTRAL" # As per user's request (1 or 2 points)
        elif overall_score == 0:
            bias_tag = "WEAK" # As per user's request (0 points)
        else: # total_score < 0
            bias_tag = "SELL"
        
        # Update component analysis dictionary
        components_analysis[component_name] = {
            "name": component_name,
            "score_breakdown": component_scores,
            "total_score": overall_score,
            "bias_tag": bias_tag,
            "summary": f"{bias_tag} bias: {'; '.join(summary_parts)}"
        }
        # ENHANCEMENT 4: Add to top_5_summary_list for final print
        top_5_summary_list.append((component_name, bias_tag))


    # Sort components by total score (descending)
    sorted_components = sorted(components_analysis.values(), key=lambda x: x["total_score"], reverse=True)

    return sorted_components[:5], top_5_summary_list # Return both


# --- Main Execution ---
if __name__ == "__main__":
    # Load all CSVs at the start
    raw_data = {}
    # Define the specific subdirectory for Natural_Gas_Data_Positioning.csv
    goscripts_info_folder = goscripts_folder / "INFO"
    goscripts_info_folder.mkdir(parents=True, exist_ok=True) # Ensure this specific subfolder exists

    for key, fname in files.items():
        if key == "lng_hist": # This is in main INFO folder
            raw_data[key] = load_clean_df(key, fname, data_folder)
        elif key == "natural_gas_data_positioning": # This is in GOScripts/INFO folder
            raw_data[key] = load_clean_df(key, fname, goscripts_info_folder)
        elif key == "plant_group_mapping": # This is in GOScripts folder
            raw_data[key] = load_clean_df(key, fname, goscripts_folder)
        else: # All other files remain in the main INFO folder
            raw_data[key] = load_clean_df(key, fname, data_folder)

    raw_data = {k: v for k, v in raw_data.items() if v is not None}

    # Extract the EIA report date for weather alignment (also used for storage positions)
    latest_eia_report_date = None
    if "eia_totals" in raw_data and "period" in raw_data["eia_totals"].columns:
        df_eia_period = raw_data["eia_totals"].dropna(subset=["period"])
        if not df_eia_period.empty:
            latest_eia_report_date = df_eia_period["period"].max().date()

    # Precompute Storage Position for all regions
    storage_positions = {}
    for d_name, i_col_name in region_map.items():
        pos_data = get_storage_position(d_name, i_col_name, raw_data, latest_eia_report_date)
        if pos_data:
            storage_positions[d_name] = pos_data

    # --- Determine the "today_analysis_date" based on the latest available data ---
    today_analysis_date = None
    prices_df_for_date_check = raw_data.get("prices")
    if prices_df_for_date_check is not None and not prices_df_for_date_check.empty and 'date' in prices_df_for_date_check.columns:
        prices_df_for_date_check['date'] = pd.to_datetime(prices_df_for_date_check['date'], errors='coerce')
        prices_df_for_date_check.dropna(subset=['date'], inplace=True)
        if not prices_df_for_date_check.empty:
            today_analysis_date = prices_df_for_date_check['date'].max().date()
        else:
            print("CRITICAL ERROR: PRICES.csv loaded but is empty or dates are invalid after cleaning. Cannot determine analysis date.")
            exit()
    else:
        print("CRITICAL ERROR: PRICES.csv not loaded or missing 'Date' column. Cannot determine analysis date.")
        exit()

    print(f"\n--- Running Natural Gas Market Component Analysis for {today_analysis_date} ---")
    print("--------------------------------------------------------------------------\n")

    # Perform the analysis
    top_components, final_recommendation_summary = analyze_components(today_analysis_date, raw_data, storage_positions)

    if top_components:
        print("\n### Top 5 Natural Gas Market Components by Score ###")
        print("--------------------------------------------------\n")
        for i, comp in enumerate(top_components):
            print(f"**{i+1}. Component: {comp['name']}**")
            print(f"   Total Score: {comp['total_score']} points")
            print(f"   📌 Bias Tag: {comp['bias_tag']}") # Print the new bias tag
            print("   Score Breakdown:")
            for signal, score in comp['score_breakdown'].items():
                print(f"     - {signal}: {score} points")
            print(f"   Summary: {comp['summary']}")
            print("\n" + "---" * 15 + "\n")
        
        # ENHANCEMENT 4: Final Polish: Add Clear Recommendation Summary
        if final_recommendation_summary:
            print("\n🔍 Recommendation Summary:")
            for component_name, bias_tag in final_recommendation_summary:
                print(f"- {component_name}: {bias_tag}")
            print("\n" + "---" * 15 + "\n")

    else:
        print("\nNo components found for analysis or insufficient data to perform calculations.")
        print("Please ensure all necessary CSV files are present, correctly named, and contain valid data.")