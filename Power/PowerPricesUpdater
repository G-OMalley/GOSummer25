from pathlib import Path
import pandas as pd
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
from gridstatusio import GridStatusClient

# === Setup Paths ===
try:
    BASE_DIR = Path(__file__).resolve().parents[1]
except NameError:
    BASE_DIR = Path(".").resolve()
    if not (BASE_DIR / "INFO").exists() and (BASE_DIR.parent / "INFO").exists():
        BASE_DIR = BASE_DIR.parent
    elif not (BASE_DIR / "INFO").exists() and (BASE_DIR.parent.parent / "INFO").exists():
        BASE_DIR = BASE_DIR.parent.parent

ENV_PATH = BASE_DIR / "Power" / ".env"
INFO_DIR = BASE_DIR / "INFO"
POWERPRICES_PATH = INFO_DIR / "PowerPrices.csv"

load_dotenv(dotenv_path=ENV_PATH)
GRIDSTATUS_API_KEY = os.getenv("GRIDSTATUS_API_KEY")

# === Define ISOs and their filtered locations ===
FILTERED_LOCATIONS = {
    "CAISO": ["DLAP_PGAE", "DLAP_SCE", "DLAP_SDGE", "DLAP_VEA", "TH_NP15", "TH_SP15", "TH_ZP26"],
    "ERCOT": ["HB_PAN", "HB_SOUTH", "HB_WEST", "LZ_AEN", "LZ_CPS", "LZ_HOUSTON", "LZ_LCRA", "LZ_NORTH", "LZ_RAYBN", "LZ_SOUTH", "LZ_WEST"],
    "NYISO": ["HUD VL", "LONGIL", "MHK VL", "MILLWD", "N", "NORTH", "NPX", "O H", "PJM", "WEST"],
    "PJM": ["AECO", "AEP", "AEP GEN HUB", "AEP-DAYTON HUB", "APS", "ATSI", "ATSI GEN HUB", "BGE", "CHICAGO GEN HUB", "CHICAGO HUB",
            "COMED", "DAY", "DEOK", "DOM", "DOMINION HUB", "DPL", "DUQ", "EASTERN HUB", "EKPC", "JCPL", "METED", "N ILLINOIS HUB",
            "NEW JERSEY HUB", "OHIO HUB", "OVEC", "PECO", "PENELEC", "PEPCO", "PJM-RTO", "PPL", "PSEG", "RECO", "WEST INT HUB", "WESTERN HUB"],
    "SPP": ["SPPNORTH_HUB", "SPPSOUTH_HUB"],
    "MISO": ["ARKANSAS.HUB", "ILLINOIS.HUB", "INDIANA.HUB", "LOUISIANA.HUB", "MICHIGAN.HUB", "MINN.HUB", "MS.HUB", "TEXAS.HUB"],
    "ISO-NE": [".H.INTERNAL_HUB", ".I.HQHIGATE120 2", ".I.HQ_P1_P2345 5", ".I.NRTHPORT138 5", ".I.ROSETON 345 1", ".I.SALBRYNB345 1",
               ".I.SHOREHAM138 99", ".Z.CONNECTICUT", ".Z.MAINE", ".Z.NEMASSBOST", ".Z.NEWHAMPSHIRE", ".Z.RHODEISLAND", ".Z.SEMASS",
               ".Z.VERMONT", ".Z.WCMASS"]
}

ISO_DATASET_MAP = {
    "CAISO": "caiso_lmp_day_ahead_hourly",
    "ERCOT": "ercot_spp_day_ahead_hourly",
    "PJM": "pjm_lmp_day_ahead_hourly",
    "MISO": "miso_lmp_day_ahead_hourly",
    "SPP": "spp_lmp_day_ahead_hourly",
    "NYISO": "nyiso_lmp_day_ahead_hourly",
    "ISO-NE": "isone_lmp_day_ahead_hourly"
}

# === Load PowerPrices ===
if POWERPRICES_PATH.exists():
    existing_df = pd.read_csv(POWERPRICES_PATH)
    existing_df["Date"] = pd.to_datetime(existing_df["Date"], errors="coerce")
else:
    existing_df = pd.DataFrame(columns=["ISO", "Date", "Location", "Max LMP ($/MWh)"])

# === Always update last 5 days ===
start_date = datetime.utcnow().date() - timedelta(days=5)
end_date = datetime.utcnow().date()

# === Initialize GridStatus Client ===
print("Starting GridStatusIO Power Price Update with filtered locations...")
client = GridStatusClient(api_key=GRIDSTATUS_API_KEY)
all_dfs = []

# === Fetch new data ===
for iso, locations in FILTERED_LOCATIONS.items():
    dataset = ISO_DATASET_MAP[iso]
    print(f"Fetching {iso} for {len(locations)} locations...")
    try:
        df = client.get_dataset(
            dataset=dataset,
            start=start_date.isoformat(),
            end=(end_date + timedelta(days=1)).isoformat(),
            filter_column="location",
            filter_value=locations,
            filter_operator="in"
        )
        if not df.empty:
            df["ISO"] = iso
            df["Date"] = pd.to_datetime(df["interval_start_utc"]).dt.date
            df["Location"] = df["location"].astype(str).str.upper().str.strip()
            if "lmp" in df.columns:
                df["LMP"] = pd.to_numeric(df["lmp"], errors="coerce")
            elif "spp" in df.columns:
                df["LMP"] = pd.to_numeric(df["spp"], errors="coerce")
            else:
                continue
            daily_max = df.groupby(["ISO", "Date", "Location"])["LMP"].max().reset_index()
            daily_max.rename(columns={"LMP": "Max LMP ($/MWh)"}, inplace=True)
            all_dfs.append(daily_max)
        else:
            print(f"No data returned for {iso}.")
    except Exception as e:
        print(f"Error fetching {iso}: {e}")

# === Merge and Write Updated Data ===
updated_data = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()

if not updated_data.empty:
    updated_data["Date"] = pd.to_datetime(updated_data["Date"])
    before_update = existing_df[existing_df["Date"] < pd.to_datetime(start_date)]
    after_update = existing_df[existing_df["Date"] > pd.to_datetime(end_date)]
    combined = pd.concat([before_update, updated_data, after_update], ignore_index=True)
    combined.drop_duplicates(subset=["ISO", "Date", "Location"], keep="last", inplace=True)
    combined.sort_values(by=["ISO", "Date", "Location"], inplace=True)
    combined.to_csv(POWERPRICES_PATH, index=False)
    print(f"✅ PowerPrices.csv updated from {start_date} to {end_date}")
else:
    print("⚠️ No new data retrieved.")

print("Done.")
input("Press Enter to exit...")
