{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoUNAZyO9aQc",
        "outputId": "18511cdb-38fa-4b36-8f54-b334779040a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gridstatusio\n",
            "  Downloading gridstatusio-0.10.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting certifi<2025,>=2024.7.4 (from gridstatusio)\n",
            "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting idna~=3.7 (from gridstatusio)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy<2,>=1.26.4 (from gridstatusio)\n",
            "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
            "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "     ----------------- ---------------------- 6.8/15.8 MB 36.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  15.7/15.8 MB 40.3 MB/s eta 0:00:01\n",
            "     --------------------------------------- 15.8/15.8 MB 38.7 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): still running...\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: pandas>1.3.0 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from gridstatusio) (2.2.3)\n",
            "Collecting requests<3,>=2.32.2 (from gridstatusio)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools<71,>=70.0.0 (from gridstatusio)\n",
            "  Downloading setuptools-70.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tabulate<0.10,>=0.9.0 (from gridstatusio)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting termcolor==1.1.0 (from gridstatusio)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: tqdm<5,>=4.66.3 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from gridstatusio) (4.67.1)\n",
            "Collecting urllib3<3,>=2.2.2 (from gridstatusio)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting virtualenv<21,>=20.26.6 (from gridstatusio)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.32.2->gridstatusio)\n",
            "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from tqdm<5,>=4.66.3->gridstatusio) (0.4.6)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21,>=20.26.6->gridstatusio)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting filelock<4,>=3.12.2 (from virtualenv<21,>=20.26.6->gridstatusio)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from virtualenv<21,>=20.26.6->gridstatusio) (4.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from pandas>1.3.0->gridstatusio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from pandas>1.3.0->gridstatusio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from pandas>1.3.0->gridstatusio) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\patri\\onedrive\\desktop\\coding\\traderhelper\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>1.3.0->gridstatusio) (1.17.0)\n",
            "Downloading gridstatusio-0.10.1-py3-none-any.whl (17 kB)\n",
            "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
            "Downloading setuptools-70.3.0-py3-none-any.whl (931 kB)\n",
            "   ---------------------------------------- 0.0/931.1 kB ? eta -:--:--\n",
            "   --------------------------------------- 931.1/931.1 kB 34.9 MB/s eta 0:00:00\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "   ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 6.1/6.1 MB 30.8 MB/s eta 0:00:00\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: termcolor, numpy\n",
            "  Building wheel for termcolor (pyproject.toml): started\n",
            "  Building wheel for termcolor (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4923 sha256=93bee8bf3b3c617373eb69a36570dbcf9542e5d23ca72e7fee0b0ca29bedae29\n",
            "  Stored in directory: c:\\users\\patri\\appdata\\local\\pip\\cache\\wheels\\0a\\a5\\07\\60ef5219c22c3223a302ea1394b51f4b50bb6fd6238fcfedca\n",
            "  Building wheel for numpy (pyproject.toml): started\n",
            "  Building wheel for numpy (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for numpy: filename=numpy-1.26.4-cp313-cp313-win_amd64.whl size=6158928 sha256=ea1f542768b8d1923f47313be4b71c729e5589ba58c37c03e029e704ce1561e3\n",
            "  Stored in directory: c:\\users\\patri\\appdata\\local\\pip\\cache\\wheels\\8b\\2d\\9f\\b6b46373f328e2ef50388915d351ccacbedac929459b5459bf\n",
            "Successfully built termcolor numpy\n",
            "Installing collected packages: termcolor, distlib, urllib3, tabulate, setuptools, numpy, idna, filelock, charset-normalizer, certifi, virtualenv, requests, gridstatusio\n",
            "\n",
            "   --- ------------------------------------  1/13 [distlib]\n",
            "   ------ ---------------------------------  2/13 [urllib3]\n",
            "   ------ ---------------------------------  2/13 [urllib3]\n",
            "   --------- ------------------------------  3/13 [tabulate]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "  Attempting uninstall: numpy\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "    Found existing installation: numpy 2.2.6\n",
            "   ------------ ---------------------------  4/13 [setuptools]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "    Uninstalling numpy-2.2.6:\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   --------------- ------------------------  5/13 [numpy]\n",
            "   ------------------ ---------------------  6/13 [idna]\n",
            "   ------------------ ---------------------  6/13 [idna]\n",
            "   ------------------------ ---------------  8/13 [charset-normalizer]\n",
            "   ------------------------ ---------------  8/13 [charset-normalizer]\n",
            "   --------------------------- ------------  9/13 [certifi]\n",
            "   ------------------------------ --------- 10/13 [virtualenv]\n",
            "   ------------------------------ --------- 10/13 [virtualenv]\n",
            "   ------------------------------ --------- 10/13 [virtualenv]\n",
            "   ------------------------------ --------- 10/13 [virtualenv]\n",
            "   ------------------------------ --------- 10/13 [virtualenv]\n",
            "   --------------------------------- ------ 11/13 [requests]\n",
            "   --------------------------------- ------ 11/13 [requests]\n",
            "   ---------------------------------------- 13/13 [gridstatusio]\n",
            "\n",
            "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.2 distlib-0.3.9 filelock-3.18.0 gridstatusio-0.10.1 idna-3.10 numpy-1.26.4 requests-2.32.3 setuptools-70.3.0 tabulate-0.9.0 termcolor-1.1.0 urllib3-2.4.0 virtualenv-20.31.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gridstatusio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PZNA1x3694RY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gridstatusio import GridStatusClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1vxdeX7p98yw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from gridstatusio import GridStatusClient # Ensure GridStatusClient is imported here\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Make sure your .env file is in the root of your TraderHelper project\n",
        "# (i.e., C:/Users/patri/OneDrive/Desktop/Coding/TraderHelper/.env)\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from the environment variable\n",
        "API_KEY = os.environ.get(\"GRIDSTATUS_API_KEY\")\n",
        "\n",
        "if API_KEY:\n",
        "    print(\"API Key loaded successfully.\") # Simplified message\n",
        "    client = GridStatusClient(api_key=API_KEY)\n",
        "else:\n",
        "    print(\"Error: GRIDSTATUS_API_KEY not found in .env file or environment variables.\")\n",
        "    print(\"Please ensure you have a .env file in your project root (TraderHelper/.env)\")\n",
        "    print(\"and it contains a line like: GRIDSTATUS_API_KEY='your_key_here'\")\n",
        "    client = None # Or raise an error: # raise ValueError(\"API Key not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_LqDBM9-fII"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "# from gridstatusio import GridStatusClient # Assumed to be imported\n",
        "# from google.colab import files # Assumed to be imported\n",
        "\n",
        "# Configuration for supported ISOs\n",
        "ISO_CONFIGS = {\n",
        "    \"CAISO\": {\n",
        "        \"dataset_id\": \"caiso_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"DLAP\", \"Trading Hub\"],\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None # No special suffix for CAISO hubs\n",
        "    },\n",
        "    \"ERCOT\": {\n",
        "        \"dataset_id\": \"ercot_spp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"Trading Hub\", \"Load Zone\"],\n",
        "        \"price_column\": \"spp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None\n",
        "    },\n",
        "    \"ISONE\": {\n",
        "        \"dataset_id\": \"isone_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"HUB\", \"EXT. NODE\", \"LOAD ZONE\"],\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None\n",
        "    },\n",
        "    \"MISO\": {\n",
        "        \"dataset_id\": \"miso_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"Hub\"],\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": \".HUB\" # MISO hubs often end with .HUB\n",
        "    },\n",
        "    \"NYISO\": {\n",
        "        \"dataset_id\": \"nyiso_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"Zone\"], # LMP.pdf also mentions \"Generator\"\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None\n",
        "    },\n",
        "    \"PJM\": {\n",
        "        \"dataset_id\": \"pjm_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"ZONE\", \"HUB\"], # PJM uses uppercase\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None\n",
        "    },\n",
        "    \"SPP\": {\n",
        "        \"dataset_id\": \"spp_lmp_day_ahead_hourly\",\n",
        "        \"location_types_for_listing\": [\"Interface\", \"Hub\"],\n",
        "        \"price_column\": \"lmp\",\n",
        "        \"market\": \"day_ahead\",\n",
        "        \"interval\": \"hourly\",\n",
        "        \"refine_hubs_suffix\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function 1: fetch_lmp_data (or generic price data fetcher)\n",
        "def fetch_lmp_data(\n",
        "    client: \"GridStatusClient\",\n",
        "    iso: str, # Used for constructing default dataset_id if override is not provided\n",
        "    market: str, # Used for constructing default dataset_id\n",
        "    interval: str, # Used for constructing default dataset_id\n",
        "    start_date_str: str,\n",
        "    end_date_str: str = None,\n",
        "    target_location_types: list[str] = None,\n",
        "    target_locations: list[str] = None,\n",
        "    limit: int = None,\n",
        "    verbose: bool = True,\n",
        "    override_dataset_id: str = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches LMP or other price data (like SPP) from the GridStatus API.\n",
        "    If override_dataset_id is provided, it's used directly. Otherwise, constructs\n",
        "    a standard LMP dataset ID (iso_lmp_market_interval).\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"DEBUG fetch_lmp_data: ENTERING FUNCTION.\")\n",
        "        print(f\"DEBUG fetch_lmp_data: Received override_dataset_id = '{override_dataset_id}'\")\n",
        "        print(f\"DEBUG fetch_lmp_data: Received iso = '{iso}', market = '{market}', interval = '{interval}'\")\n",
        "\n",
        "    if override_dataset_id and override_dataset_id.strip():\n",
        "        dataset_id = override_dataset_id.strip()\n",
        "        if verbose:\n",
        "            print(f\"Using override_dataset_ID: '{dataset_id}'\")\n",
        "    else:\n",
        "        # Default construction if no override (primarily for LMP datasets)\n",
        "        dataset_id = f\"{iso.lower()}_lmp_{market.lower()}_{interval.lower()}\"\n",
        "        if verbose:\n",
        "            print(f\"Constructed dataset ID (as override_dataset_id was None/empty): '{dataset_id}'\")\n",
        "\n",
        "    try:\n",
        "        start_dt = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        if verbose:\n",
        "            print(f\"Error: Invalid start_date_str format: {start_date_str}. Please use YYYY-MM-DD.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if end_date_str is None:\n",
        "        end_dt = start_dt + timedelta(days=1)\n",
        "        actual_end_date_str = end_dt.strftime(\"%Y-%m-%d\")\n",
        "        if verbose:\n",
        "            print(f\"No end_date_str provided, API will fetch data for the full day of {start_date_str} (API end: {actual_end_date_str}).\")\n",
        "    else:\n",
        "        try:\n",
        "            end_dt = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
        "            if end_dt <= start_dt: # API end date must be after start date\n",
        "                if verbose:\n",
        "                    print(f\"Error: end_date_str ({end_date_str}) for API must be after start_date_str ({start_date_str}).\")\n",
        "                return pd.DataFrame()\n",
        "            actual_end_date_str = end_date_str\n",
        "        except ValueError:\n",
        "            if verbose:\n",
        "                print(f\"Error: Invalid end_date_str format: {end_date_str}. Please use YYYY-MM-DD.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    api_filter_column = None\n",
        "    api_filter_value = None\n",
        "    api_filter_operator = None\n",
        "\n",
        "    # Prioritize target_locations if both are provided\n",
        "    if target_locations:\n",
        "        api_filter_column = \"location\"\n",
        "        api_filter_value = target_locations if isinstance(target_locations, list) else [target_locations]\n",
        "        api_filter_operator = \"in\" if isinstance(target_locations, list) and len(target_locations) > 0 else \"=\"\n",
        "    elif target_location_types:\n",
        "        api_filter_column = \"location_type\"\n",
        "        api_filter_value = target_location_types if isinstance(target_location_types, list) else [target_location_types]\n",
        "        api_filter_operator = \"in\" if isinstance(target_location_types, list) and len(target_location_types) > 0 else \"=\"\n",
        "\n",
        "    if verbose and api_filter_column:\n",
        "        print(f\"API Filter: column='{api_filter_column}', operator='{api_filter_operator}', value(s)='{api_filter_value}'\")\n",
        "\n",
        "    df_result = pd.DataFrame()\n",
        "    try:\n",
        "        if verbose:\n",
        "            print(f\"Fetching data for '{dataset_id}' from {start_date_str} (inclusive) to {actual_end_date_str} (exclusive)...\")\n",
        "\n",
        "        data_response = client.get_dataset(\n",
        "            dataset=dataset_id,\n",
        "            start=start_date_str,\n",
        "            end=actual_end_date_str,\n",
        "            filter_column=api_filter_column,\n",
        "            filter_value=api_filter_value,\n",
        "            filter_operator=api_filter_operator,\n",
        "            limit=limit\n",
        "        )\n",
        "\n",
        "        if isinstance(data_response, pd.DataFrame):\n",
        "            df_result = data_response\n",
        "        elif isinstance(data_response, dict) and \"data\" in data_response: # Should be rare now\n",
        "            df_result = pd.DataFrame(data_response[\"data\"])\n",
        "        elif isinstance(data_response, list): # Also should be rare\n",
        "            df_result = pd.DataFrame(data_response)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"Unexpected data format received from API: {type(data_response)}\")\n",
        "\n",
        "        if df_result.empty and verbose:\n",
        "            print(\"No data retrieved for the given parameters.\")\n",
        "        elif verbose and not df_result.empty:\n",
        "            print(f\"Successfully fetched and processed {len(df_result)} rows.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error fetching or processing data for '{dataset_id}': {e}\")\n",
        "    return df_result\n",
        "\n",
        "# Function 2: get_iso_filtered_locations\n",
        "def get_iso_filtered_locations(\n",
        "    client: \"GridStatusClient\",\n",
        "    iso: str,\n",
        "    market: str, # For context if dataset_id_for_locations is None\n",
        "    interval: str, # For context\n",
        "    target_location_types: list[str],\n",
        "    lookback_days: int = 30,\n",
        "    limit_per_type: int = 2000,\n",
        "    verbose: bool = True,\n",
        "    dataset_id_for_locations: str = None\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Fetches unique location names for specified ISO, market component (types),\n",
        "    using a specific dataset ID for location discovery.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"DEBUG get_iso_filtered_locations: Using dataset_id_for_locations = '{dataset_id_for_locations}'\")\n",
        "        print(f\"Fetching unique {iso} locations for types {target_location_types} from the last {lookback_days} days...\")\n",
        "\n",
        "    end_lookback_dt = datetime.now(timezone.utc)\n",
        "    start_lookback_dt = end_lookback_dt - timedelta(days=lookback_days)\n",
        "\n",
        "    start_lookback_str = start_lookback_dt.strftime(\"%Y-%m-%d\")\n",
        "    # API end_date is exclusive, so add 1 day to include the full end_lookback_dt\n",
        "    api_end_lookback_str = (end_lookback_dt + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    all_locations = set()\n",
        "\n",
        "    for loc_type in target_location_types:\n",
        "        if verbose:\n",
        "            print(f\"\\nFetching locations for ISO: {iso}, Type: {loc_type} using dataset: {dataset_id_for_locations or 'default constructed'}...\")\n",
        "\n",
        "        df_type_locations = fetch_lmp_data(\n",
        "            client=client,\n",
        "            iso=iso,\n",
        "            market=market,\n",
        "            interval=interval,\n",
        "            start_date_str=start_lookback_str,\n",
        "            end_date_str=api_end_lookback_str,\n",
        "            target_location_types=[loc_type], # Filter by one type at a time\n",
        "            limit=limit_per_type,\n",
        "            verbose=verbose,\n",
        "            override_dataset_id=dataset_id_for_locations\n",
        "        )\n",
        "        if not df_type_locations.empty and \"location\" in df_type_locations.columns:\n",
        "            unique_for_type = df_type_locations[\"location\"].unique()\n",
        "            all_locations.update(unique_for_type)\n",
        "            if verbose:\n",
        "                print(f\"Found {len(unique_for_type)} unique locations for type '{loc_type}'.\")\n",
        "        elif verbose:\n",
        "            print(f\"No locations found or 'location' column missing for ISO '{iso}', type '{loc_type}'.\")\n",
        "\n",
        "    if not all_locations:\n",
        "        if verbose:\n",
        "            print(f\"\\nNo locations found for ISO '{iso}' matching types {target_location_types} in the lookback period using dataset {dataset_id_for_locations or 'default'}.\")\n",
        "        return []\n",
        "\n",
        "    sorted_locations = sorted(list(all_locations))\n",
        "    if verbose:\n",
        "        print(f\"\\nTotal unique locations found for ISO '{iso}', types {target_location_types}: {len(sorted_locations)}\")\n",
        "    return sorted_locations\n",
        "\n",
        "# Function 3: interactive_iso_price_explorer_module\n",
        "def interactive_iso_price_explorer_module(client: \"GridStatusClient\"):\n",
        "    \"\"\"\n",
        "    Interactively prompts user for ISO, market component (location), dates,\n",
        "    and then charts daily max price and/or offers CSV download.\n",
        "    \"\"\"\n",
        "    print(\"--- Interactive ISO Price Explorer ---\")\n",
        "    module_verbose = True # For detailed logging within this module and its calls\n",
        "\n",
        "    # 1. Get ISO from user\n",
        "    print(\"\\nAvailable ISOs:\")\n",
        "    iso_options = list(ISO_CONFIGS.keys())\n",
        "    for i, iso_name in enumerate(iso_options):\n",
        "        print(f\"{i+1}. {iso_name}\")\n",
        "\n",
        "    selected_iso = None\n",
        "    while True:\n",
        "        try:\n",
        "            iso_choice_input = input(\"Enter the number or name of the ISO: \")\n",
        "            try:\n",
        "                iso_idx = int(iso_choice_input) -1\n",
        "                if 0 <= iso_idx < len(iso_options):\n",
        "                    selected_iso = iso_options[iso_idx]\n",
        "                    break\n",
        "            except ValueError:\n",
        "                if iso_choice_input.upper() in iso_options:\n",
        "                    selected_iso = iso_choice_input.upper()\n",
        "                    break\n",
        "            print(\"Invalid ISO selection. Please choose from the list.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nOperation cancelled.\")\n",
        "            return\n",
        "    print(f\"You selected ISO: {selected_iso}\")\n",
        "\n",
        "    config = ISO_CONFIGS[selected_iso]\n",
        "    dataset_id = config[\"dataset_id\"]\n",
        "    location_types_for_listing = config[\"location_types_for_listing\"]\n",
        "    price_column = config[\"price_column\"]\n",
        "    market = config[\"market\"]\n",
        "    interval = config[\"interval\"]\n",
        "    refine_suffix = config.get(\"refine_hubs_suffix\") # e.g., \".HUB\" for MISO\n",
        "\n",
        "    # 2. Get and display available locations for the chosen ISO and types\n",
        "    if module_verbose:\n",
        "        print(f\"\\nDEBUG: Calling get_iso_filtered_locations for {selected_iso} with dataset_id_for_locations = '{dataset_id}' and types = {location_types_for_listing}\")\n",
        "\n",
        "    initial_locations = get_iso_filtered_locations(\n",
        "        client=client,\n",
        "        iso=selected_iso,\n",
        "        market=market,\n",
        "        interval=interval,\n",
        "        target_location_types=location_types_for_listing,\n",
        "        lookback_days=30, # Look back 30 days for recent locations\n",
        "        verbose=module_verbose,\n",
        "        dataset_id_for_locations=dataset_id # Use the specific dataset ID from config\n",
        "    )\n",
        "\n",
        "    available_locations_for_user = initial_locations\n",
        "    if selected_iso == \"MISO\" and refine_suffix: # Special handling for MISO .HUB refinement\n",
        "        refined = [loc for loc in initial_locations if isinstance(loc, str) and loc.endswith(refine_suffix)]\n",
        "        if refined:\n",
        "            if module_verbose:\n",
        "                print(f\"Refining MISO Hub list: {len(initial_locations)} total found, {len(refined)} end with '{refine_suffix}'.\")\n",
        "            available_locations_for_user = refined\n",
        "        elif module_verbose:\n",
        "            print(f\"No MISO Hubs ending with '{refine_suffix}' found. Presenting all initially found.\")\n",
        "\n",
        "    if not available_locations_for_user:\n",
        "        print(f\"Could not retrieve a list of {selected_iso} locations for types {location_types_for_listing} using dataset {dataset_id}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nAvailable {selected_iso} Locations ({', '.join(location_types_for_listing)} from '{dataset_id}'):\")\n",
        "    for i, loc_name in enumerate(available_locations_for_user):\n",
        "        print(f\"{i+1}. {loc_name}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # 3. Get user selected location\n",
        "    selected_location_name = None\n",
        "    while True:\n",
        "        try:\n",
        "            loc_choice_input = input(f\"Enter the number or full name of the {selected_iso} location: \")\n",
        "            try:\n",
        "                loc_idx = int(loc_choice_input) - 1\n",
        "                if 0 <= loc_idx < len(available_locations_for_user):\n",
        "                    selected_location_name = available_locations_for_user[loc_idx]\n",
        "                    break\n",
        "            except ValueError:\n",
        "                if loc_choice_input in available_locations_for_user:\n",
        "                    selected_location_name = loc_choice_input\n",
        "                    break\n",
        "            print(\"Invalid location. Please choose from the list.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nOperation cancelled.\")\n",
        "            return\n",
        "    print(f\"You selected location: {selected_location_name}\")\n",
        "\n",
        "    # 4. Get dates from user\n",
        "    while True:\n",
        "        try:\n",
        "            start_date_input = input(\"Enter the start date (YYYY-MM-DD): \")\n",
        "            datetime.strptime(start_date_input, \"%Y-%m-%d\")\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nOperation cancelled.\")\n",
        "            return\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            end_date_input = input(\"Enter the end date (YYYY-MM-DD, inclusive): \")\n",
        "            end_dt_obj = datetime.strptime(end_date_input, \"%Y-%m-%d\")\n",
        "            start_dt_obj = datetime.strptime(start_date_input, \"%Y-%m-%d\")\n",
        "            if end_dt_obj < start_dt_obj:\n",
        "                print(\"End date cannot be before the start date.\")\n",
        "            else:\n",
        "                break\n",
        "        except ValueError:\n",
        "            print(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nOperation cancelled.\")\n",
        "            return\n",
        "\n",
        "    # API end_date is exclusive, so add one day to include the user's end_date_input\n",
        "    api_end_date_for_fetch = (datetime.strptime(end_date_input, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # 5. Get output preference\n",
        "    output_choice = \"\"\n",
        "    while output_choice not in ['1', '2', '3']:\n",
        "        try:\n",
        "            output_choice = input(\"\\nChoose output:\\n1. Chart Only\\n2. CSV Download Only\\n3. Chart and CSV Download\\nEnter choice (1, 2, or 3): \")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nOperation cancelled.\")\n",
        "            return\n",
        "        if output_choice not in ['1', '2', '3']:\n",
        "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "    # 6. Fetch data\n",
        "    print(f\"\\nFetching {market.replace('_',' ').title()} {interval.title()} {price_column.upper()} data for {selected_iso} location '{selected_location_name}' from {start_date_input} to {end_date_input}...\")\n",
        "\n",
        "    # For fetching data for a specific location, we use target_locations\n",
        "    price_df = fetch_lmp_data(\n",
        "        client=client,\n",
        "        iso=selected_iso,\n",
        "        market=market,\n",
        "        interval=interval,\n",
        "        start_date_str=start_date_input,\n",
        "        end_date_str=api_end_date_for_fetch,\n",
        "        target_locations=[selected_location_name],\n",
        "        verbose=module_verbose,\n",
        "        override_dataset_id=dataset_id\n",
        "    )\n",
        "\n",
        "    if price_df.empty:\n",
        "        print(f\"No {price_column.upper()} data found for {selected_iso} location '{selected_location_name}' in the specified date range using dataset '{dataset_id}'.\")\n",
        "        return\n",
        "\n",
        "    # 7. Process data and output\n",
        "    try:\n",
        "        # Ensure correct data types for price and time\n",
        "        if price_column not in price_df.columns:\n",
        "            print(f\"Error: Price column '{price_column}' not found. Available columns: {price_df.columns.tolist()}\")\n",
        "            return\n",
        "        price_df[price_column] = pd.to_numeric(price_df[price_column], errors='coerce')\n",
        "\n",
        "        time_col = None\n",
        "        if 'interval_start_utc' in price_df.columns:\n",
        "            time_col = 'interval_start_utc'\n",
        "        elif 'time' in price_df.columns: # Some ISOs might use 'time'\n",
        "            time_col = 'time'\n",
        "\n",
        "        if not time_col:\n",
        "            print(\"Error: Suitable time column ('interval_start_utc' or 'time') not found.\")\n",
        "            return\n",
        "\n",
        "        price_df[time_col] = pd.to_datetime(price_df[time_col], errors='coerce')\n",
        "        price_df = price_df.dropna(subset=[price_column, time_col])\n",
        "\n",
        "        if price_df.empty:\n",
        "            print(\"Data became empty after cleaning (e.g., price or time values were not valid).\")\n",
        "            return\n",
        "\n",
        "        # Charting\n",
        "        if output_choice in ['1', '3']:\n",
        "            price_df_for_chart = price_df.set_index(time_col)\n",
        "            daily_max_price = price_df_for_chart[price_column].resample('D').max().dropna()\n",
        "\n",
        "            if daily_max_price.empty:\n",
        "                print(\"No daily maximum price data to plot after resampling.\")\n",
        "            else:\n",
        "                plt.figure(figsize=(15, 7))\n",
        "                daily_max_price.plot(marker='o', linestyle='-')\n",
        "                plt.title(f\"Daily Maximum {market.replace('_',' ').title()} {interval.title()} {price_column.upper()} for {selected_iso} Location:\\n{selected_location_name}\\n({start_date_input} to {end_date_input})\", fontsize=14)\n",
        "                plt.xlabel(\"Date\", fontsize=12)\n",
        "                plt.ylabel(f\"Maximum {price_column.upper()} ($/MWh)\", fontsize=12)\n",
        "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                plt.xticks(rotation=45, ha=\"right\")\n",
        "                plt.tight_layout()\n",
        "\n",
        "                ax = plt.gca()\n",
        "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "                num_days_plot = (datetime.strptime(end_date_input, \"%Y-%m-%d\") - datetime.strptime(start_date_input, \"%Y-%m-%d\")).days\n",
        "                if num_days_plot <= 14:\n",
        "                     ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "                elif num_days_plot <= 70 :\n",
        "                    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
        "                else:\n",
        "                    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "                plt.show()\n",
        "\n",
        "        # CSV Download\n",
        "        if output_choice in ['2', '3']:\n",
        "            # Download the full fetched data for the selected location and dates\n",
        "            csv_filename = f\"{selected_iso}_{selected_location_name.replace('.','_')}_{price_column}_{start_date_input}_to_{end_date_input}.csv\"\n",
        "            price_df.to_csv(csv_filename, index=False)\n",
        "            #files.download(csv_filename) # This is a Colab specific function\n",
        "            print(f\"\\nCSV file '{csv_filename}' prepared for download.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data processing, charting, or download: {e}\")\n",
        "\n",
        "# --- Example of how to call it in a new Colab cell after defining these functions ---\n",
        "# Ensure 'client' is initialized with your API_KEY\n",
        "# interactive_iso_price_explorer_module(client)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0psapaEj_Blw",
        "outputId": "cfa6eb67-4e8a-432c-d97c-c48e12fdeae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Interactive ISO Price Explorer ---\n",
            "\n",
            "Available ISOs:\n",
            "1. CAISO\n",
            "2. ERCOT\n",
            "3. ISONE\n",
            "4. MISO\n",
            "5. NYISO\n",
            "6. PJM\n",
            "7. SPP\n"
          ]
        }
      ],
      "source": [
        "interactive_iso_price_explorer_module(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You entered: \n"
          ]
        }
      ],
      "source": [
        "test_response = input(\"VS Code Input Test - Type something and press Enter: \")\n",
        "print(f\"You entered: {test_response}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
